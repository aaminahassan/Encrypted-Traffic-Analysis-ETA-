{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10987457,"sourceType":"datasetVersion","datasetId":6838633},{"sourceId":12026450,"sourceType":"datasetVersion","datasetId":5657923},{"sourceId":12027240,"sourceType":"datasetVersion","datasetId":7567192},{"sourceId":12310325,"sourceType":"datasetVersion","datasetId":7734818},{"sourceId":12332031,"sourceType":"datasetVersion","datasetId":4421597},{"sourceId":12333153,"sourceType":"datasetVersion","datasetId":7774544}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cheikhsadibousidibe/etbet-lstm?scriptVersionId=264590550\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# generation of dataset for finetuning","metadata":{}},{"cell_type":"markdown","source":"## importations","metadata":{}},{"cell_type":"code","source":"!pip install scapy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport binascii\nimport pandas as pd\nimport numpy as np\nimport tqdm\nimport scapy.all as scapy\nfrom sklearn.model_selection import train_test_split\nfrom scapy.all import rdpcap, wrpcap,PcapReader\nfrom collections import defaultdict,Counter\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom multiprocessing import Pool, cpu_count\nfrom scapy.layers.inet import IP, TCP, UDP\nimport hashlib\nimport time\nimport psutil\nimport gc\nimport csv\n# Scapy imports\nfrom scapy.all import rdpcap, wrpcap\nfrom scapy.layers.inet import IP, TCP, UDP, ICMP\nfrom scapy.layers.l2 import ARP\nfrom scapy.layers.dhcp import DHCP\nfrom scapy.layers.dns import DNS\nfrom scapy.layers.inet6 import IPv6\nfrom pathlib import Path\nimport shutil\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for dirpath,dirname, file in os.walk(\"/kaggle/input/pcap-data/ustf_tfc\"):\n    print(dirpath)\n    print(dirname)\n    print(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:59:06.644038Z","iopub.execute_input":"2025-06-16T15:59:06.644427Z","iopub.status.idle":"2025-06-16T15:59:06.668826Z","shell.execute_reply.started":"2025-06-16T15:59:06.644396Z","shell.execute_reply":"2025-06-16T15:59:06.667865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#reorganizer ISCXVPN files \ndef organize_pcap_files(input_directories, output_directory, final_labels=None, file_label_map=None):\n    \"\"\"\n    Organize PCAP files from input directories into labeled subdirectories.\n    \n    Args:\n        input_directories (list): List of input directory paths containing PCAP files\n        output_directory (str): Output directory where organized files will be placed\n        final_labels (list): Target label categories (default: [\"browsing\", \"chat\", \"email\", \"ftp\", \"p2p\", \"streaming\", \"voip\"])\n        file_label_map (dict): Explicit mapping of filenames to labels\n    \n    Returns:\n        dict: Summary of files organized by category\n    \"\"\"\n    if final_labels is None:\n        final_labels = [\"browsing\", \"chat\", \"email\", \"ftp\", \"p2p\", \"streaming\", \"voip\"]\n    \n    # Use provided file label mapping or default mapping\n    if file_label_map is None:\n        file_label_map = {\n            # --- voip ---\n            \"vpn_facebook_audio2.pcap\": \"voip\",\n            \"vpn_hangouts_audio2.pcap\": \"voip\",\n            \"vpn_hangouts_audio1.pcap\": \"voip\",\n            \"vpn_skype_audio2.pcap\": \"voip\",\n            \"vpn_skype_audio1.pcap\": \"voip\",\n            \"vpn_voipbuster1a.pcap\": \"voip\",\n            \"vpn_voipbuster1b.pcap\": \"voip\",\n            # --- chat ---\n            \"vpn_hangouts_chat1a.pcap\": \"chat\",\n            \"vpn_hangouts_chat1b.pcap\": \"chat\",\n            \"vpn_facebook_chat1a.pcap\": \"chat\",\n            \"vpn_facebook_chat1b.pcap\": \"chat\",\n            \"vpn_aim_chat1a.pcap\": \"chat\",\n            \"vpn_aim_chat1b.pcap\": \"chat\",\n            \"vpn_skype_chat1a.pcap\": \"chat\",\n            \"vpn_skype_chat1b.pcap\": \"chat\",\n            \"vpn_icq_chat1a.pcap\": \"chat\",\n            \"vpn_icq_chat1b.pcap\": \"chat\",\n            # --- email ---\n            \"vpn_email2a.pcap\": \"email\",\n            \"vpn_email2b.pcap\": \"email\",\n            # --- ftp ---\n            \"vpn_ftps_A.pcap\": \"ftp\",\n            \"vpn_ftps_B.pcap\": \"ftp\",\n            \"vpn_sftp_A.pcap\": \"ftp\",\n            \"vpn_sftp_B.pcap\": \"ftp\",\n            \"vpn_skype_files1a.pcap\": \"ftp\",\n            \"vpn_skype_files1b.pcap\": \"ftp\",\n            # --- streaming ---\n            \"vpn_vimeo_A.pcap\": \"streaming\",\n            \"vpn_vimeo_B.pcap\": \"streaming\",\n            \"vpn_youtube_A.pcap\": \"streaming\",\n            \"vpn_spotify_A.pcap\": \"streaming\",\n            \"vpn_netflix_A.pcap\": \"streaming\",\n            # --- p2p ---\n            \"vpn_bittorrent.pcap\": \"p2p\"\n        }\n    \n    # Initialize results\n    results = defaultdict(list)\n    skipped_files = []\n    \n    print(f\"ðŸš€ Starting PCAP file organization\")\n    print(f\"ðŸ“ Target labels: {final_labels}\")\n    print(f\"ðŸ“‚ Output directory: {output_directory}\")\n    \n    # Create output directory and subdirectories\n    os.makedirs(output_directory, exist_ok=True)\n    for label in final_labels:\n        label_dir = os.path.join(output_directory, label)\n        os.makedirs(label_dir, exist_ok=True)\n        print(f\"ðŸ“ Created directory: {label_dir}\")\n    \n    # Process each input directory\n    total_processed = 0\n    total_copied = 0\n    \n    for input_dir in input_directories:\n        if not os.path.exists(input_dir):\n            print(f\"âš ï¸ Input directory does not exist: {input_dir}\")\n            continue\n            \n        print(f\"\\nðŸ“‚ Processing directory: {input_dir}\")\n        \n        # Get all PCAP files in the directory\n        pcap_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.pcap', '.pcapng'))]\n        print(f\"ðŸ“„ Found {len(pcap_files)} PCAP files\")\n        \n        for filename in pcap_files:\n            total_processed += 1\n            source_path = os.path.join(input_dir, filename)\n            \n            # Look up label directly from the mapping\n            matched_label = file_label_map.get(filename)\n            \n            if matched_label and matched_label in final_labels:\n                # Copy file to appropriate subdirectory\n                dest_dir = os.path.join(output_directory, matched_label)\n                dest_path = os.path.join(dest_dir, filename)\n                \n                try:\n                    shutil.copy2(source_path, dest_path)\n                    results[matched_label].append(filename)\n                    total_copied += 1\n                    print(f\"âœ… {filename} â†’ {matched_label}/\")\n                except Exception as e:\n                    print(f\"âŒ Error copying {filename}: {e}\")\n                    skipped_files.append((filename, f\"Copy error: {e}\"))\n            else:\n                skipped_files.append((filename, \"No matching label found\"))\n                print(f\"â­ï¸ Skipped: {filename} (no clear category match)\")\n    \n    # Print summary\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸ“Š ORGANIZATION SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"ðŸ“ˆ Total files processed: {total_processed}\")\n    print(f\"âœ… Total files copied: {total_copied}\")\n    print(f\"â­ï¸ Total files skipped: {len(skipped_files)}\")\n    \n    print(f\"\\nðŸ“ Files by category:\")\n    for label in final_labels:\n        count = len(results[label])\n        print(f\"  {label}: {count} files\")\n        if count > 0:\n            # Show first few files as examples\n            examples = results[label][:3]\n            for example in examples:\n                print(f\"    ðŸ“„ {example}\")\n            if len(results[label]) > 3:\n                print(f\"    ... and {len(results[label]) - 3} more\")\n    \n    if skipped_files:\n        print(f\"\\nâ­ï¸ Skipped files:\")\n        for filename, reason in skipped_files[:10]:  # Show first 10\n            print(f\"  ðŸ“„ {filename}: {reason}\")\n        if len(skipped_files) > 10:\n            print(f\"  ... and {len(skipped_files) - 10} more\")\n    \n    return dict(results)\n\ndef get_file_category_mapping():\n    \"\"\"\n    Return the exact mapping used for categorizing files.\n    This helps understand how files are being classified.\n    \"\"\"\n    return {\n        # --- voip ---\n        \"vpn_facebook_audio2.pcap\": \"voip\",\n        \"vpn_hangouts_audio2.pcap\": \"voip\",\n        \"vpn_hangouts_audio1.pcap\": \"voip\",\n        \"vpn_skype_audio2.pcap\": \"voip\",\n        \"vpn_skype_audio1.pcap\": \"voip\",\n        \"vpn_voipbuster1a.pcap\": \"voip\",\n        \"vpn_voipbuster1b.pcap\": \"voip\",\n        # --- chat ---\n        \"vpn_hangouts_chat1a.pcap\": \"chat\",\n        \"vpn_hangouts_chat1b.pcap\": \"chat\",\n        \"vpn_facebook_chat1a.pcap\": \"chat\",\n        \"vpn_facebook_chat1b.pcap\": \"chat\",\n        \"vpn_aim_chat1a.pcap\": \"chat\",\n        \"vpn_aim_chat1b.pcap\": \"chat\",\n        \"vpn_skype_chat1a.pcap\": \"chat\",\n        \"vpn_skype_chat1b.pcap\": \"chat\",\n        \"vpn_icq_chat1a.pcap\": \"chat\",\n        \"vpn_icq_chat1b.pcap\": \"chat\",\n        # --- email ---\n        \"vpn_email2a.pcap\": \"email\",\n        \"vpn_email2b.pcap\": \"email\",\n        # --- ftp ---\n        \"vpn_ftps_A.pcap\": \"ftp\",\n        \"vpn_ftps_B.pcap\": \"ftp\",\n        \"vpn_sftp_A.pcap\": \"ftp\",\n        \"vpn_sftp_B.pcap\": \"ftp\",\n        \"vpn_skype_files1a.pcap\": \"ftp\",\n        \"vpn_skype_files1b.pcap\": \"ftp\",\n        # --- streaming ---\n        \"vpn_vimeo_A.pcap\": \"streaming\",\n        \"vpn_vimeo_B.pcap\": \"streaming\",\n        \"vpn_youtube_A.pcap\": \"streaming\",\n        \"vpn_spotify_A.pcap\": \"streaming\",\n        \"vpn_netflix_A.pcap\": \"streaming\",\n        # --- p2p ---\n        \"vpn_bittorrent.pcap\": \"p2p\"\n    }\n\n# Example usage for your specific case:\ndef organize_kaggle_pcaps(custom_file_label_map=None):\n    \"\"\"\n    Specific function for organizing the Kaggle PCAP files.\n    \n    Args:\n        custom_file_label_map (dict): Optional custom mapping to override defaults\n    \"\"\"\n    input_dirs = [\n        \"/kaggle/input/pcap-data/ISCX-VPN-PCAPS-APP\",\n        \"/kaggle/input/pcap-data/ISCX-VPN-PCAPs-02\"\n    ]\n    \n    output_dir = \"/kaggle/working/ISCX_VPN_pcap\"\n    \n    final_labels = [\"browsing\", \"chat\", \"email\", \"ftp\", \"p2p\", \"streaming\", \"voip\"]\n    \n    results = organize_pcap_files(\n        input_directories=input_dirs,\n        output_directory=output_dir,\n        final_labels=final_labels,\n        file_label_map=custom_file_label_map\n    )\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T12:59:59.145435Z","iopub.execute_input":"2025-06-14T12:59:59.146585Z","iopub.status.idle":"2025-06-14T12:59:59.166913Z","shell.execute_reply.started":"2025-06-14T12:59:59.146551Z","shell.execute_reply":"2025-06-14T12:59:59.166003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom collections import defaultdict\n\ndef organize_ustf_tfc_dataset(ustf_tfc_path, output_directory):\n    \"\"\"\n    Organize USTF-TFC dataset into 20 labeled subdirectories.\n    \n    Args:\n        ustf_tfc_path (str): Path to the USTF-TFC dataset (/kaggle/input/pcap-data/ustf_tfc)\n        output_directory (str): Output directory where organized files will be placed\n    \n    Returns:\n        dict: Summary of files organized by category\n    \"\"\"\n    \n    # Define the 20 target categories\n    target_labels = [\n        # Benign applications (13 categories)\n        \"outlook\", \"worldofwarcraft\", \"bittorrent\", \"skype\", \"gmail\", \n        \"facetime\", \"ftp\", \"mysql\", \"smb\", \"weibo\",\n        # Malware (10 categories) \n        \"tinba\", \"miuref\", \"zeus\", \"htbot\", \"nsis_ay\", \"neris\", \n        \"geodo\", \"cridex\", \"shifu\", \"virut\"\n    ]\n    \n    # File mapping for USTF-TFC dataset\n    file_label_map = {\n        # Benign Applications (in /Benign/ directory)\n        \"Outlook.pcap\": \"outlook\",\n        \"WorldOfWarcraft.pcap\": \"worldofwarcraft\", \n        \"BitTorrent.pcap\": \"bittorrent\",\n        \"Skype.pcap\": \"skype\",\n        \"Gmail.pcap\": \"gmail\",\n        \"Facetime.pcap\": \"facetime\",\n        \"FTP.pcap\": \"ftp\",\n        \"MySQL.pcap\": \"mysql\",\n        \"SMB-1.pcap\": \"smb\",\n        \"SMB-2.pcap\": \"smb\", \n        \"Weibo-1.pcap\": \"weibo\",\n        \"Weibo-2.pcap\": \"weibo\",\n        \"Weibo-3.pcap\": \"weibo\",\n        \"Weibo-4.pcap\": \"weibo\",\n        \n        # Malware (in /Malware/ directory and subdirectories)\n        \"Tinba.pcap\": \"tinba\",\n        \"Miuref.pcap\": \"miuref\", \n        \"Zeus.pcap\": \"zeus\",\n        \"Htbot.pcap\": \"htbot\",\n        \"Nsis-ay.pcap\": \"nsis_ay\",\n        \"Neris.pcap\": \"neris\",\n        \"Geodo.pcap\": \"geodo\", \n        \"Cridex.pcap\": \"cridex\",\n        \"Shifu.pcap\": \"shifu\",\n        \"Virut.pcap\": \"virut\"\n    }\n    \n    print(f\"ðŸš€ Starting USTF-TFC dataset organization\")\n    print(f\"ðŸ“‚ Source: {ustf_tfc_path}\")\n    print(f\"ðŸ“ Output: {output_directory}\")\n    print(f\"ðŸŽ¯ Target categories: {len(target_labels)}\")\n    \n    # Create output directory and subdirectories\n    os.makedirs(output_directory, exist_ok=True)\n    for label in target_labels:\n        label_dir = os.path.join(output_directory, label)\n        os.makedirs(label_dir, exist_ok=True)\n    \n    results = defaultdict(list)\n    skipped_files = []\n    total_processed = 0\n    total_copied = 0\n    \n    # Function to recursively find and process PCAP files\n    def process_directory(directory_path, base_path=\"\"):\n        nonlocal total_processed, total_copied\n        \n        for root, dirs, files in os.walk(directory_path):\n            for filename in files:\n                if filename.lower().endswith(('.pcap', '.pcapng')):\n                    total_processed += 1\n                    source_path = os.path.join(root, filename)\n                    \n                    # Look up label from mapping\n                    matched_label = file_label_map.get(filename)\n                    \n                    if matched_label and matched_label in target_labels:\n                        # Copy file to appropriate subdirectory\n                        dest_dir = os.path.join(output_directory, matched_label)\n                        dest_path = os.path.join(dest_dir, filename)\n                        \n                        try:\n                            shutil.copy2(source_path, dest_path)\n                            results[matched_label].append(filename)\n                            total_copied += 1\n                            \n                            # Show path for context\n                            relative_path = os.path.relpath(source_path, ustf_tfc_path)\n                            print(f\"âœ… {relative_path} â†’ {matched_label}/\")\n                        except Exception as e:\n                            print(f\"âŒ Error copying {filename}: {e}\")\n                            skipped_files.append((filename, f\"Copy error: {e}\"))\n                    else:\n                        skipped_files.append((filename, \"No matching label found\"))\n                        relative_path = os.path.relpath(source_path, ustf_tfc_path)\n                        print(f\"â­ï¸ Skipped: {relative_path} (no mapping)\")\n    \n    # Process the USTF-TFC dataset\n    if os.path.exists(ustf_tfc_path):\n        print(f\"\\nðŸ“‚ Processing USTF-TFC dataset...\")\n        process_directory(ustf_tfc_path)\n    else:\n        print(f\"âŒ USTF-TFC path does not exist: {ustf_tfc_path}\")\n        return {}\n    \n    # Print summary\n    print(f\"\\n{'='*70}\")\n    print(f\"ðŸ“Š USTF-TFC ORGANIZATION SUMMARY\")\n    print(f\"{'='*70}\")\n    print(f\"ðŸ“ˆ Total files processed: {total_processed}\")\n    print(f\"âœ… Total files copied: {total_copied}\")\n    print(f\"â­ï¸ Total files skipped: {len(skipped_files)}\")\n    \n    print(f\"\\nðŸ“ Files by category:\")\n    for label in target_labels:\n        count = len(results[label])\n        print(f\"  {label}: {count} files\")\n        if count > 0:\n            # Show all files for small counts, first few for larger counts\n            examples = results[label][:3] if len(results[label]) > 3 else results[label]\n            for example in examples:\n                print(f\"    ðŸ“„ {example}\")\n            if len(results[label]) > 3:\n                print(f\"    ... and {len(results[label]) - 3} more\")\n    \n    if skipped_files:\n        print(f\"\\nâ­ï¸ Skipped files:\")\n        for filename, reason in skipped_files:\n            print(f\"  ðŸ“„ {filename}: {reason}\")\n    \n    return dict(results)\n\ndef preview_ustf_tfc_organization(ustf_tfc_path):\n    \"\"\"\n    Preview how USTF-TFC files would be organized without actually copying them.\n    \n    Args:\n        ustf_tfc_path (str): Path to the USTF-TFC dataset\n    \n    Returns:\n        dict: Preview of organization results\n    \"\"\"\n    file_label_map = {\n        # Benign Applications\n        \"Outlook.pcap\": \"outlook\",\n        \"WorldOfWarcraft.pcap\": \"worldofwarcraft\", \n        \"BitTorrent.pcap\": \"bittorrent\",\n        \"Skype.pcap\": \"skype\",\n        \"Gmail.pcap\": \"gmail\",\n        \"Facetime.pcap\": \"facetime\",\n        \"FTP.pcap\": \"ftp\",\n        \"MySQL.pcap\": \"mysql\",\n        \"SMB-1.pcap\": \"smb\",\n        \"SMB-2.pcap\": \"smb\", \n        \"Weibo-1.pcap\": \"weibo\",\n        \"Weibo-2.pcap\": \"weibo\",\n        \"Weibo-3.pcap\": \"weibo\",\n        \"Weibo-4.pcap\": \"weibo\",\n        \n        # Malware\n        \"Tinba.pcap\": \"tinba\",\n        \"Miuref.pcap\": \"miuref\", \n        \"Zeus.pcap\": \"zeus\",\n        \"Htbot.pcap\": \"htbot\",\n        \"Nsis-ay.pcap\": \"nsis_ay\",\n        \"Neris.pcap\": \"neris\",\n        \"Geodo.pcap\": \"geodo\", \n        \"Cridex.pcap\": \"cridex\",\n        \"Shifu.pcap\": \"shifu\",\n        \"Virut.pcap\": \"virut\"\n    }\n    \n    preview_results = defaultdict(list)\n    \n    if not os.path.exists(ustf_tfc_path):\n        print(f\"âŒ Path does not exist: {ustf_tfc_path}\")\n        return {}\n    \n    for root, dirs, files in os.walk(ustf_tfc_path):\n        for filename in files:\n            if filename.lower().endswith(('.pcap', '.pcapng')):\n                matched_label = file_label_map.get(filename)\n                \n                if matched_label:\n                    preview_results[matched_label].append(filename)\n                else:\n                    preview_results['skipped'].append(filename)\n    \n    return dict(preview_results)\n\ndef organize_ustf_tfc_kaggle():\n    \"\"\"\n    Specific function for organizing USTF-TFC dataset in Kaggle environment.\n    \"\"\"\n    ustf_tfc_path = \"/kaggle/input/pcap-data/ustf_tfc\"\n    output_dir = \"/kaggle/working/organized_ustf_tfc\"\n    \n    results = organize_ustf_tfc_dataset(\n        ustf_tfc_path=ustf_tfc_path,\n        output_directory=output_dir\n    )\n    \n    return results\n\ndef get_ustf_tfc_mapping():\n    \"\"\"\n    Return the exact mapping used for USTF-TFC files.\n    \"\"\"\n    return {\n        # Benign Applications\n        \"Outlook.pcap\": \"outlook\",\n        \"WorldOfWarcraft.pcap\": \"worldofwarcraft\", \n        \"BitTorrent.pcap\": \"bittorrent\",\n        \"Skype.pcap\": \"skype\",\n        \"Gmail.pcap\": \"gmail\",\n        \"Facetime.pcap\": \"facetime\",\n        \"FTP.pcap\": \"ftp\",\n        \"MySQL.pcap\": \"mysql\",\n        \"SMB-1.pcap\": \"smb\",\n        \"SMB-2.pcap\": \"smb\", \n        \"Weibo-1.pcap\": \"weibo\",\n        \"Weibo-2.pcap\": \"weibo\",\n        \"Weibo-3.pcap\": \"weibo\",\n        \"Weibo-4.pcap\": \"weibo\",\n        \n        # Malware\n        \"Tinba.pcap\": \"tinba\",\n        \"Miuref.pcap\": \"miuref\", \n        \"Zeus.pcap\": \"zeus\",\n        \"Htbot.pcap\": \"htbot\",\n        \"Nsis-ay.pcap\": \"nsis_ay\",\n        \"Neris.pcap\": \"neris\",\n        \"Geodo.pcap\": \"geodo\", \n        \"Cridex.pcap\": \"cridex\",\n        \"Shifu.pcap\": \"shifu\",\n        \"Virut.pcap\": \"virut\"\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T16:02:45.213502Z","iopub.execute_input":"2025-06-16T16:02:45.213919Z","iopub.status.idle":"2025-06-16T16:02:45.253948Z","shell.execute_reply.started":"2025-06-16T16:02:45.213893Z","shell.execute_reply":"2025-06-16T16:02:45.252939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Flow level treatment ","metadata":{}},{"cell_type":"code","source":"def get_memory_usage():\n    \"\"\"Get current memory usage in MB\"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / 1024 / 1024 \n\ndef create_session_key(packet):\n    \"\"\"\n    Create a session key for flow grouping based on 5-tuple (src_ip, src_port, dst_ip, dst_port, protocol).\n    Handles bidirectionality by sorting IP/port pairs.\n    \"\"\"\n    try:\n        if packet.haslayer('IP'):\n            ip_layer = packet['IP']\n            src_ip = ip_layer.src\n            dst_ip = ip_layer.dst\n            \n            if packet.haslayer('TCP'):\n                src_port = packet['TCP'].sport\n                dst_port = packet['TCP'].dport\n                protocol = 'TCP'\n            elif packet.haslayer('UDP'):\n                src_port = packet['UDP'].sport\n                dst_port = packet['UDP'].dport\n                protocol = 'UDP'\n            else:\n                # For non-TCP/UDP IP packets (e.g., ICMP), use protocol number and 0 for ports\n                src_port = dst_port = 0\n                protocol = str(ip_layer.proto) # Use IP protocol number\n                \n            # Create a bidirectional session key by sorting the (IP, Port) tuples\n            # This ensures that A->B and B->A traffic for the same flow gets the same key\n            if (src_ip, src_port) < (dst_ip, dst_port):\n                return f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{protocol}\"\n            else:\n                return f\"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{protocol}\"\n        else:\n            # For non-IP packets (e.g., ARP, Ethernet-only), create a key based on a hash of the packet\n            # This is a fallback and might group dissimilar non-IP packets, but ensures all packets are considered.\n            return f\"non_ip-{hashlib.md5(bytes(packet)).hexdigest()[:8]}\"\n    except Exception as e:\n        # Fallback for any parsing errors, using a hash of the raw packet bytes for uniqueness\n        print(f\"Error creating session key: {e}. Falling back to packet hash.\")\n        return f\"error-{hashlib.md5(bytes(packet)).hexdigest()[:8]}\"\n\ndef extract_flows_streaming(pcap_file, min_packets=5, max_flows_per_batch=100, \n                            memory_limit_mb=1000, max_total_flows=None, timeout_seconds=300):\n    \"\"\"\n    Extract flows from large PCAP files using a streaming approach with smart flow detection.\n    Efficiently handles flows by using timeout-based cleanup and memory limits.\n\n    Args:\n        pcap_file (str): Path to the input PCAP file.\n        min_packets (int): Minimum number of packets a flow must have to be considered valid.\n        max_flows_per_batch (int): Maximum number of completed flows to accumulate before a batch processing/cleanup.\n        memory_limit_mb (int): Memory usage threshold in MB to trigger a cleanup/batch processing.\n        max_total_flows (int, optional): Maximum total flows to extract from this PCAP file.\n        timeout_seconds (int): Time in seconds after which a flow is considered complete if no new packets are seen.\n\n    Returns:\n        list: A list of lists, where each inner list contains packets belonging to a single valid flow.\n    \"\"\"\n    print(f\"Processing large PCAP file: {pcap_file}\")\n    print(f\"Memory limit: {memory_limit_mb}MB, Max flows per batch: {max_flows_per_batch}\")\n    print(f\"Min packets per flow: {min_packets}, Flow timeout: {timeout_seconds}s\")\n    \n    try:\n        sessions = defaultdict(list)  # Stores active flows (session_key -> list of packets)\n        session_last_seen = {}       # Tracks the timestamp of the last packet for each session\n        valid_flows = []             # Stores completed flows that meet min_packets criteria\n        \n        packet_count = 0\n        batch_count = 0\n        total_extracted_flows = 0\n        discarded_short_flows = 0\n        \n        with PcapReader(pcap_file) as pcap_reader:\n            start_memory = get_memory_usage()\n            last_cleanup_time = time.time()\n            \n            for packet in pcap_reader:\n                packet_count += 1\n                current_real_time = time.time() # Wall clock time for periodic checks\n                packet_timestamp = packet.time  # Scapy's packet timestamp for flow timeout\n                \n                session_key = create_session_key(packet)\n                sessions[session_key].append(packet)\n                session_last_seen[session_key] = packet_timestamp\n                \n                # Periodic cleanup of old/incomplete sessions\n                # This helps manage memory and finalize flows that have ended\n                if (packet_count % 5000 == 0 or \n                    current_real_time - last_cleanup_time > 30): # Cleanup every 5000 packets or 30 seconds\n                    \n                    sessions_to_cleanup = []\n                    added_this_cleanup_cycle = 0\n                    discarded_this_cleanup_cycle = 0\n                    \n                    # Iterate through active sessions to find those that have timed out\n                    for session_key, last_timestamp in list(session_last_seen.items()): # Use list to allow deletion during iteration\n                        time_since_last_packet = packet_timestamp - last_timestamp\n                        \n                        if time_since_last_packet > timeout_seconds:\n                            session_packet_count = len(sessions[session_key])\n                            \n                            if session_packet_count >= min_packets:\n                                # This flow is complete and meets minimum packet requirement\n                                session_packets = sessions[session_key]\n                                session_packets.sort(key=lambda p: p.time) # Ensure packets are sorted by time\n                                valid_flows.append(session_packets)\n                                total_extracted_flows += 1\n                                added_this_cleanup_cycle += 1\n                            else:\n                                # Discard flow with insufficient packets\n                                discarded_short_flows += 1\n                                discarded_this_cleanup_cycle += 1\n                                \n                            sessions_to_cleanup.append(session_key)\n                    \n                    # Remove cleaned up sessions from active tracking dictionaries\n                    for key in sessions_to_cleanup:\n                        del sessions[key]\n                        del session_last_seen[key]\n                    \n                    if sessions_to_cleanup:\n                        print(f\"Cleanup: Added {added_this_cleanup_cycle} flows, \"\n                              f\"discarded {discarded_this_cleanup_cycle} short flows. Active sessions remaining: {len(sessions)}\")\n                    \n                    last_cleanup_time = current_real_time\n                \n                # Check memory usage and batch processing periodically\n                if packet_count % 1000 == 0:\n                    current_memory = get_memory_usage()\n                    memory_used = current_memory - start_memory\n                    \n                    # Count sessions that currently have enough packets to be considered valid\n                    potential_valid_sessions = len([s for s in sessions.values() if len(s) >= min_packets])\n                    \n                    print(f\"Processed {packet_count} packets, Memory: {memory_used:.1f}MB, \"\n                          f\"Active sessions: {len(sessions)}, Potential flows: {potential_valid_sessions}, \"\n                          f\"Completed flows (total): {total_extracted_flows}, Discarded (total): {discarded_short_flows}\")\n                    \n                    # If memory usage is high or we have accumulated enough completed flows,\n                    # clear the `valid_flows` list to free memory and reset tracking.\n                    if (memory_used > memory_limit_mb or \n                        len(valid_flows) >= max_flows_per_batch or\n                        potential_valid_sessions >= max_flows_per_batch):\n                        \n                        batch_count += 1\n                        print(f\"Batch {batch_count}: Processing {len(valid_flows)} completed flows (accumulated)\")\n                        \n                        # If a max_total_flows limit is set, check if we've reached it.\n                        # This break ensures we don't process more than needed from this file.\n                        if max_total_flows and total_extracted_flows >= max_total_flows:\n                            print(f\"Reached maximum total flows limit for this file: {max_total_flows}\")\n                            # Ensure valid_flows doesn't exceed the limit if we break early\n                            valid_flows = valid_flows[:max_total_flows - (total_extracted_flows - len(valid_flows))]\n                            break # Exit the packet reading loop\n                        \n                        # Clear accumulated valid flows from memory, as they will be written to disk soon\n                        if len(valid_flows) >= max_flows_per_batch: # Only clear if we actually have a batch to process\n                            valid_flows.clear()\n                        \n                        # Force garbage collection to ensure memory is freed\n                        gc.collect()\n                        \n                        # Reset memory tracking for the next batch\n                        start_memory = get_memory_usage()\n            \n            # Final processing of any remaining active sessions after all packets are read\n            print(\"Processing remaining sessions after end of file...\")\n            final_flows_added = 0\n            final_flows_discarded = 0\n            \n            for session_key, session_packets in sessions.items():\n                if len(session_packets) >= min_packets:\n                    # If max_total_flows is set, check before adding more flows\n                    if max_total_flows and total_extracted_flows >= max_total_flows:\n                        print(f\"Reached maximum total flows limit ({max_total_flows}) during final processing.\")\n                        break # Stop adding flows\n                    session_packets.sort(key=lambda p: p.time)\n                    valid_flows.append(session_packets)\n                    total_extracted_flows += 1\n                    final_flows_added += 1\n                else:\n                    final_flows_discarded += 1\n            \n            print(f\"Final processing: Added {final_flows_added} flows, \"\n                  f\"discarded {final_flows_discarded} short flows.\")\n            print(f\"Total flows extracted from {pcap_file}: {total_extracted_flows}\")\n            print(f\"Total short flows discarded from {pcap_file}: {discarded_short_flows + final_flows_discarded}\")\n            \n            # Clear all session data from memory\n            sessions.clear()\n            session_last_seen.clear()\n            gc.collect()\n            \n            # Return flows, respecting the max_total_flows limit if set\n            return valid_flows[:max_total_flows] if max_total_flows else valid_flows\n            \n    except Exception as e:\n        print(f\"Error processing {pcap_file}: {e}\")\n        return []\n\ndef write_flows_in_chunks(flows, output_dir, base_filename, chunk_size=50):\n    \"\"\"\n    Write flows to files in chunks to avoid memory issues when dealing with many flows.\n\n    Args:\n        flows (list): A list of flow packets (each flow is a list of Scapy packets).\n        output_dir (str): Directory where flow PCAP files will be saved.\n        base_filename (str): Base name for the output flow files (derived from original PCAP).\n        chunk_size (int): Number of flows to process and write in each chunk.\n\n    Returns:\n        int: The total number of flows successfully written to disk.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    written_count = 0\n    \n    print(f\"Starting to write {len(flows)} flows to {output_dir} in chunks of {chunk_size}...\")\n    \n    # Iterate through flows in specified chunk sizes\n    for i in tqdm.tqdm(range(0, len(flows), chunk_size), desc=\"Writing flows\"):\n        chunk_flows = flows[i:i + chunk_size]\n        \n        # Write each flow within the current chunk\n        for j, flow_packets in enumerate(chunk_flows):\n            flow_index = i + j\n            # Create a unique hash for the filename to prevent collisions, especially if multiple PCAPs\n            # contribute to the same label and flow index.\n            session_hash = hashlib.md5(f\"{base_filename}_{flow_index}_{time.time_ns()}_{random.randint(0, 10000)}\".encode()).hexdigest()[:8]\n            output_file = os.path.join(output_dir, f\"{base_filename}_flow_{flow_index:06d}_{session_hash}.pcap\")\n            \n            try:\n                wrpcap(output_file, flow_packets)\n                written_count += 1\n            except Exception as e:\n                print(f\"Error writing flow {flow_index} to {output_file}: {e}\")\n        \n        # Explicitly delete the chunk and force garbage collection to free memory\n        del chunk_flows\n        gc.collect()\n        \n        print(f\"Written {written_count} flows so far for {base_filename}...\")\n    \n    print(f\"Finished writing flows. Total written: {written_count}\")\n    return written_count\n\ndef process_large_pcap_efficiently(pcap_file, output_dir, label_name, \n                                     max_flows_needed=1000, min_packets=5,\n                                     memory_limit_mb=800, max_flows_per_batch=100,\n                                     flow_timeout=300):\n    \"\"\"\n    Process a single large PCAP file, extract flows, and write them to disk.\n    This function acts as an orchestrator for large files.\n\n    Args:\n        pcap_file (str): Path to the input PCAP file.\n        output_dir (str): Base output directory.\n        label_name (str): Label associated with the PCAP file (e.g., 'malware', 'benign').\n        max_flows_needed (int): Maximum number of flows to extract from this specific PCAP file.\n        min_packets (int): Minimum number of packets a flow must have.\n        memory_limit_mb (int): Memory threshold for streaming extraction.\n        max_flows_per_batch (int): Batch size for flow accumulation during streaming.\n        flow_timeout (int): Timeout for flow completion in seconds.\n\n    Returns:\n        int: The number of flows successfully written for this PCAP file.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Processing large PCAP: {os.path.basename(pcap_file)}\")\n    print(f\"Target flows from this file: {max_flows_needed}\")\n    print(f\"{'='*60}\")\n    \n    start_time = time.time()\n    \n    # Extract flows using the streaming approach\n    flows = extract_flows_streaming(\n        pcap_file=pcap_file,\n        min_packets=min_packets,\n        max_flows_per_batch=max_flows_per_batch,\n        memory_limit_mb=memory_limit_mb,\n        max_total_flows=max_flows_needed, # Pass the limit to the extractor\n        timeout_seconds=flow_timeout\n    )\n    \n    if not flows:\n        print(f\"No valid flows found in {pcap_file} after extraction.\")\n        return 0\n    \n    print(f\"Extracted {len(flows)} flows from {pcap_file} in {time.time() - start_time:.1f} seconds.\")\n    \n    # Shuffle the extracted flows to get a random sample if more were extracted than needed\n    if len(flows) > max_flows_needed:\n        random.shuffle(flows)\n        flows_to_write = flows[:max_flows_needed]\n        print(f\"Limiting to {len(flows_to_write)} flows for writing (due to max_flows_needed).\")\n    else:\n        flows_to_write = flows\n    \n    # Determine the output directory for this label\n    base_filename = os.path.splitext(os.path.basename(pcap_file))[0]\n    label_output_dir = os.path.join(output_dir, label_name)\n    \n    # Write the selected flows to disk in chunks\n    written_count = write_flows_in_chunks(\n        flows=flows_to_write,\n        output_dir=label_output_dir,\n        base_filename=base_filename,\n        chunk_size=50  # Write 50 flows at a time to manage memory\n    )\n    \n    # Clear all flow data from memory after writing\n    flows.clear()\n    flows_to_write.clear()\n    gc.collect()\n    \n    print(f\"Successfully wrote {written_count} flows for label '{label_name}' from {os.path.basename(pcap_file)}.\")\n    return written_count\n\ndef process_regular_pcap_file(pcap_file, output_dir, label_name, max_flows_needed, min_packets=5):\n    \"\"\"\n    Process a regular-sized PCAP file by loading all packets into memory,\n    extracting flows, and writing them to disk.\n    This is suitable for smaller PCAP files where full loading is feasible.\n\n    Args:\n        pcap_file (str): Path to the input PCAP file.\n        output_dir (str): Base output directory.\n        label_name (str): Label associated with the PCAP file.\n        max_flows_needed (int): Maximum number of flows to extract from this specific PCAP file.\n        min_packets (int): Minimum number of packets a flow must have.\n\n    Returns:\n        int: The number of flows successfully written for this PCAP file.\n    \"\"\"\n    print(f\"\\nðŸ“„ Processing regular file: {os.path.basename(pcap_file)}\")\n    print(f\"Target flows from this file: {max_flows_needed}\")\n    \n    try:\n        # Load all packets into memory (suitable for smaller files)\n        packets = rdpcap(pcap_file)\n        if len(packets) == 0:\n            print(f\"No packets found in {pcap_file}.\")\n            return 0\n            \n        # Group packets by session key\n        sessions = defaultdict(list)\n        for packet in packets:\n            session_key = create_session_key(packet)\n            sessions[session_key].append(packet)\n        \n        # Filter for valid flows (meeting min_packets criteria)\n        valid_flows = []\n        for session_key, session_packets in sessions.items():\n            if len(session_packets) >= min_packets:\n                session_packets.sort(key=lambda p: p.time) # Sort packets within the flow by timestamp\n                valid_flows.append(session_packets)\n        \n        # Limit the number of flows and shuffle for random sampling if more are available\n        if len(valid_flows) > max_flows_needed:\n            print(f\"Found {len(valid_flows)} flows, sampling down to {max_flows_needed}.\")\n            valid_flows = random.sample(valid_flows, max_flows_needed)\n        else:\n            print(f\"Found {len(valid_flows)} flows, writing all of them.\")\n            \n        # Determine the output directory for this label\n        base_filename = os.path.splitext(os.path.basename(pcap_file))[0]\n        label_output_dir = os.path.join(output_dir, label_name)\n        os.makedirs(label_output_dir, exist_ok=True)\n        \n        written_count = 0\n        # Write each valid flow to a separate PCAP file\n        for i, flow_packets in enumerate(valid_flows):\n            # Create a unique hash for the filename\n            session_hash = hashlib.md5(f\"{pcap_file}_{i}_{time.time_ns()}_{random.randint(0, 10000)}\".encode()).hexdigest()[:8]\n            output_file = os.path.join(label_output_dir, f\"{base_filename}_flow_{i:04d}_{session_hash}.pcap\")\n            \n            try:\n                wrpcap(output_file, flow_packets)\n                written_count += 1\n            except Exception as e:\n                print(f\"Error writing flow {i} to {output_file}: {e}\")\n        \n        # Clear memory after processing\n        del packets\n        sessions.clear()\n        valid_flows.clear()\n        gc.collect()\n\n        return written_count\n        \n    except Exception as e:\n        print(f\"Error processing {pcap_file}: {e}\")\n        return 0\n\ndef process_directory_with_large_files(input_dir, output_dir, samples_per_label=2000, # Changed default to 2000 as per your requirement\n                                       min_packets=3, # Changed default to 3 as per your requirement\n                                       memory_limit_mb=800, \n                                       large_file_threshold_mb=500):\n    \"\"\"\n    Process a directory containing PCAP files, organizing them by label (subdirectories).\n    It intelligently chooses between streaming and full-load processing based on file size.\n\n    Args:\n        input_dir (str): The root directory containing label subdirectories with PCAP files.\n        output_dir (str): The base directory where processed flows will be saved.\n        samples_per_label (int): The maximum number of flow files to generate for each label.\n        min_packets (int): The minimum number of packets a flow must contain.\n        memory_limit_mb (int): Memory threshold for activating streaming mode for large files.\n        large_file_threshold_mb (int): File size in MB above which streaming mode is used.\n\n    Returns:\n        dict: A dictionary showing the total number of flows extracted for each label.\n    \"\"\"\n    total_processed = {}\n    \n    # Iterate through each label directory within the input directory\n    for label_name in os.listdir(input_dir):\n        label_path = os.path.join(input_dir, label_name)\n        if not os.path.isdir(label_path):\n            continue # Skip if it's not a directory\n            \n        print(f\"\\nðŸ·ï¸ Processing label: {label_name}\")\n        \n        # Collect all PCAP files for the current label\n        pcap_files = []\n        for root, dirs, files in os.walk(label_path):\n            for file in files:\n                if file.lower().endswith(('.pcap', '.pcapng')):\n                    file_path = os.path.join(root, file)\n                    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n                    pcap_files.append((file_path, file_size_mb))\n        \n        if not pcap_files:\n            print(f\"No PCAP files found for {label_name}. Skipping.\")\n            continue\n        \n        # Sort files by size (smaller files first) to efficiently meet samples_per_label\n        pcap_files.sort(key=lambda x: x[1])\n        \n        print(f\"Found {len(pcap_files)} PCAP files for {label_name}:\")\n        for file_path, size_mb in pcap_files:\n            print(f\"  ðŸ“ {os.path.basename(file_path)}: {size_mb:.1f}MB\")\n        \n        # Process files until the required number of samples per label is met\n        extracted_count = 0\n        for file_path, file_size_mb in pcap_files:\n            if extracted_count >= samples_per_label:\n                print(f\"Reached target {samples_per_label} flows for {label_name}. Skipping remaining files.\")\n                break # Stop processing files for this label if target is met\n            \n            remaining_needed = samples_per_label - extracted_count\n            \n            if file_size_mb > large_file_threshold_mb:\n                print(f\"âš ï¸ Large file detected: {os.path.basename(file_path)} ({file_size_mb:.1f}MB). Using streaming mode.\")\n                # Use streaming approach for very large files\n                flows_extracted = process_large_pcap_efficiently(\n                    pcap_file=file_path,\n                    output_dir=output_dir,\n                    label_name=label_name,\n                    max_flows_needed=remaining_needed, # Only extract up to the remaining needed flows\n                    min_packets=min_packets,\n                    memory_limit_mb=memory_limit_mb\n                )\n            else:\n                print(f\"ðŸ“„ Processing regular file: {os.path.basename(file_path)} ({file_size_mb:.1f}MB). Using standard mode.\")\n                # Use regular approach for smaller files\n                flows_extracted = process_regular_pcap_file(\n                    pcap_file=file_path,\n                    output_dir=output_dir,\n                    label_name=label_name,\n                    max_flows_needed=remaining_needed, # Only extract up to the remaining needed flows\n                    min_packets=min_packets\n                )\n            \n            extracted_count += flows_extracted\n            print(f\"âœ… Progress for {label_name}: {extracted_count}/{samples_per_label} flows extracted so far.\")\n            \n            # Force garbage collection between processing different files\n            gc.collect()\n            \n        total_processed[label_name] = extracted_count\n        print(f\"ðŸŽ¯ Completed {label_name}: {extracted_count} flows extracted.\")\n    \n    return total_processed\n\ndef verify_flows_with_min_packets(pcap_file, min_packets=3):\n    \"\"\"\n    Verifies and counts the number of flows in a PCAP file that meet a minimum packet count.\n    This function is designed to quickly assess the potential number of extractable flows\n    without writing them to disk.\n\n    Args:\n        pcap_file (str): Path to the input PCAP file.\n        min_packets (int): The minimum number of packets a flow must have to be counted.\n\n    Returns:\n        int: The total count of flows that satisfy the min_packets criteria.\n    \"\"\"\n    print(f\"\\nðŸ”¬ Verifying flows in {os.path.basename(pcap_file)} with at least {min_packets} packets...\")\n    \n    sessions = defaultdict(list)\n    flow_count = 0\n    \n    try:\n        with PcapReader(pcap_file) as pcap_reader:\n            for packet in pcap_reader:\n                session_key = create_session_key(packet)\n                sessions[session_key].append(packet) # Just append, no need for full packet objects if only counting\n        \n        for session_key, session_packets in sessions.items():\n            if len(session_packets) >= min_packets:\n                flow_count += 1\n        \n        print(f\"âœ… Verification complete: Found {flow_count} flows in {os.path.basename(pcap_file)} meeting the {min_packets}-packet criterion.\")\n        return flow_count\n    except Exception as e:\n        print(f\"Error verifying {pcap_file}: {e}\")\n        return 0\n\n# Kaggle-optimized usage example\ndef process_pcaps_for_kaggle(input_path, output_path, samples_per_label=2000): # Updated default\n    \"\"\"\n    Main function optimized for a Kaggle-like environment.\n    This function serves as the entry point for running the PCAP processing.\n\n    Args:\n        input_path (str): Path to the directory containing PCAP files, organized by label.\n        output_path (str): Path to the directory where the processed flow files will be saved.\n        samples_per_label (int): The target number of flow files for each label.\n    \"\"\"\n    print(\"ðŸš€ Starting PCAP processing optimized for Kaggle\")\n    print(f\"ðŸ“Š Memory available: {psutil.virtual_memory().available / (1024**3):.1f}GB\")\n    print(f\"ðŸŽ¯ Target samples per label: {samples_per_label}\")\n    \n    # Use conservative memory settings for Kaggle environments, which often have limited RAM\n    results = process_directory_with_large_files(\n        input_dir=input_path,\n        output_dir=output_path,\n        samples_per_label=samples_per_label,\n        min_packets=3,  # Set to 3 as per your requirement\n        memory_limit_mb=800,  # Conservative memory limit for Kaggle\n        large_file_threshold_mb=300 # Lower threshold to use streaming earlier for Kaggle\n    )\n    \n    print(\"\\nðŸ“ˆ Final Results:\")\n    total_flows = 0\n    for label, count in results.items():\n        print(f\"  {label}: {count} flows\")\n        total_flows += count\n    \n    print(f\"ðŸŽ‰ Total flows extracted across all labels: {total_flows}\")\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:06:29.826714Z","iopub.execute_input":"2025-06-03T16:06:29.827169Z","iopub.status.idle":"2025-06-03T16:06:29.889885Z","shell.execute_reply.started":"2025-06-03T16:06:29.827142Z","shell.execute_reply":"2025-06-03T16:06:29.888693Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Packet level ","metadata":{}},{"cell_type":"code","source":"def get_memory_usage():\n    \"\"\"Get current memory usage in MB\"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / 1024 / 1024\n\ndef cut(obj, sec):\n    \"\"\"Cut object into sections\"\"\"\n    if not obj or len(obj) == 0:\n        return []\n        \n    result = [obj[i:i+sec] for i in range(0,len(obj),sec)]\n    \n    if not result or len(result) == 0:\n        return []\n        \n    remanent_count = len(result[0])%4\n    if remanent_count == 0:\n        pass\n    else:\n        result = [obj[i:i+sec+remanent_count] for i in range(0,len(obj),sec+remanent_count)]\n    \n    return result\n\ndef bigram_generation(packet_datagram, packet_len=64, flag=True):\n    \"\"\"Generate bigrams from packet data\"\"\"\n    if not packet_datagram or len(packet_datagram) == 0:\n        return ''\n    \n    result = ''\n    generated_datagram = cut(packet_datagram, 1)\n    \n    if not generated_datagram or len(generated_datagram) == 0:\n        return ''\n    \n    token_count = 0\n    for sub_string_index in range(len(generated_datagram)):\n        if sub_string_index != (len(generated_datagram) - 1):\n            token_count += 1\n            if token_count > packet_len:\n                break\n            else:\n                merge_word_bigram = generated_datagram[sub_string_index] + generated_datagram[sub_string_index + 1]\n        else:\n            break\n        result += merge_word_bigram\n        result += ' '\n    \n    return result\n\ndef size_format(size):\n    \"\"\"Format file size in KB\"\"\"\n    return '%.3f' % float(size/1000)\n\ndef is_valid_packet(packet, min_size_bytes=80):\n    \"\"\"\n    Check if a packet is valid for processing.\n    \n    Args:\n        packet: Scapy packet object\n        min_size_bytes: Minimum packet size in bytes\n    \n    Returns:\n        bool: True if packet is valid, False otherwise\n    \"\"\"\n    try:\n        # Check packet size\n        if len(packet) < min_size_bytes:\n            return False\n            \n        # Check if it's an IP packet (IPv4 or IPv6)\n        if not (packet.haslayer('IP') or packet.haslayer('IPv6')):\n            return False\n            \n        # Filter out common non-data protocols\n        if (packet.haslayer('ARP') or packet.haslayer('DNS') or \n            packet.haslayer('DHCP') or packet.haslayer('ICMPv6') or\n            packet.haslayer('ICMP')):\n            return False\n            \n        return True\n    except:\n        return False\n\ndef extract_packet_feature(packet, payload_len=128):\n    \"\"\"\n    Extract features from a single packet.\n    \n    Args:\n        packet: Scapy packet object\n        payload_len: Maximum payload length to process\n    \n    Returns:\n        str: Processed packet data as bigram string\n    \"\"\"\n    try:\n        packet_data = packet.copy()\n        data = binascii.hexlify(bytes(packet_data))\n        packet_string = data.decode()\n        \n        # Skip Ethernet header (first 76 characters = 38 bytes)\n        new_packet_string = packet_string[76:] if len(packet_string) > 76 else packet_string\n        \n        # Generate bigrams\n        result = bigram_generation(new_packet_string, packet_len=payload_len, flag=True)\n        return result\n    except Exception as e:\n        print(f\"Error extracting packet feature: {e}\")\n        return ''\n\ndef extract_packets_streaming(pcap_file, max_packets=5000, memory_limit_mb=1000, \n                            min_packet_size=80, payload_len=128):\n    \"\"\"\n    Extract packets from large PCAP files using streaming approach.\n    \n    Args:\n        pcap_file (str): Path to the input PCAP file\n        max_packets (int): Maximum number of packets to extract\n        memory_limit_mb (int): Memory usage threshold in MB\n        min_packet_size (int): Minimum packet size in bytes\n        payload_len (int): Payload length for feature extraction\n    \n    Returns:\n        list: List of valid packet objects (not features)\n    \"\"\"\n    print(f\"Processing large PCAP file: {pcap_file}\")\n    print(f\"Memory limit: {memory_limit_mb}MB, Max packets: {max_packets}\")\n    print(f\"Min packet size: {min_packet_size} bytes\")\n    \n    try:\n        valid_packets = []  # Store actual packet objects instead of features\n        packet_count = 0\n        valid_packet_count = 0\n        batch_size = 1000\n        \n        with PcapReader(pcap_file) as pcap_reader:\n            start_memory = get_memory_usage()\n            \n            for packet in pcap_reader:\n                packet_count += 1\n                \n                # Check if packet is valid for processing\n                if not is_valid_packet(packet, min_packet_size):\n                    continue\n                \n                # Store the actual packet object instead of extracting features\n                valid_packets.append(packet.copy())  # Use copy() to ensure packet is preserved\n                valid_packet_count += 1\n                \n                # Check if we've reached the maximum number of packets\n                if valid_packet_count >= max_packets:\n                    print(f\"Reached maximum packet limit: {max_packets}\")\n                    break\n                \n                # Memory and progress check every batch\n                if packet_count % batch_size == 0:\n                    current_memory = get_memory_usage()\n                    memory_used = current_memory - start_memory\n                    \n                    print(f\"Processed {packet_count} packets, Valid: {valid_packet_count}, \"\n                          f\"Memory: {memory_used:.1f}MB\")\n                    \n                    # If memory usage is too high, break\n                    if memory_used > memory_limit_mb:\n                        print(f\"Memory limit reached ({memory_limit_mb}MB), stopping extraction\")\n                        break\n                    \n                    # Force garbage collection periodically\n                    if packet_count % (batch_size * 5) == 0:\n                        gc.collect()\n            \n            print(f\"Extraction complete: {valid_packet_count} valid packets from {packet_count} total packets\")\n            return valid_packets\n            \n    except Exception as e:\n        print(f\"Error processing {pcap_file}: {e}\")\n        return []\n\ndef write_packets_in_chunks(valid_packets, output_dir, base_filename, chunk_size=100):\n    \"\"\"\n    Write packet objects to PCAP files in chunks.\n    \n    Args:\n        valid_packets (list): List of Scapy packet objects\n        output_dir (str): Directory to write PCAP files\n        base_filename (str): Base name for output files\n        chunk_size (int): Number of packets to process in each chunk\n    \n    Returns:\n        int: Number of packets successfully written\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    written_count = 0\n    \n    print(f\"Writing {len(valid_packets)} packets to {output_dir} in chunks of {chunk_size}...\")\n    \n    # Write packets in chunks to manage memory\n    for i in tqdm.tqdm(range(0, len(valid_packets), chunk_size), desc=\"Writing packets\"):\n        chunk_packets = valid_packets[i:i + chunk_size]\n        \n        # Write each packet to a separate PCAP file\n        for j, packet in enumerate(chunk_packets):\n            packet_index = i + j\n            # Create unique filename\n            packet_hash = hashlib.md5(f\"{base_filename}_{packet_index}_{time.time_ns()}_{random.randint(0, 10000)}\".encode()).hexdigest()[:8]\n            output_file = os.path.join(output_dir, f\"{base_filename}_packet_{packet_index:06d}_{packet_hash}.pcap\")\n            \n            try:\n                # Write single packet to PCAP file\n                wrpcap(output_file, [packet])\n                written_count += 1\n            except Exception as e:\n                print(f\"Error writing packet {packet_index} to {output_file}: {e}\")\n        \n        # Clean up chunk from memory\n        del chunk_packets\n        gc.collect()\n        \n        if i % (chunk_size * 10) == 0:  # Progress update every 10 chunks\n            print(f\"Written {written_count} packets so far for {base_filename}...\")\n    \n    print(f\"Finished writing packets. Total written: {written_count}\")\n    return written_count\n\ndef process_large_pcap_packet_level(pcap_file, output_dir, label_name, \n                                   max_packets_needed=5000, min_packet_size=80,\n                                   memory_limit_mb=800, payload_len=128):\n    \"\"\"\n    Process a single large PCAP file for packet-level analysis.\n    \n    Args:\n        pcap_file (str): Path to input PCAP file\n        output_dir (str): Base output directory\n        label_name (str): Label for the PCAP file\n        max_packets_needed (int): Maximum packets to extract\n        min_packet_size (int): Minimum packet size in bytes\n        memory_limit_mb (int): Memory threshold\n        payload_len (int): Payload length for processing (unused now but kept for compatibility)\n    \n    Returns:\n        int: Number of packets successfully processed\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Processing large PCAP for packets: {os.path.basename(pcap_file)}\")\n    print(f\"Target packets from this file: {max_packets_needed}\")\n    print(f\"{'='*60}\")\n    \n    start_time = time.time()\n    \n    # Extract valid packets using streaming approach\n    valid_packets = extract_packets_streaming(\n        pcap_file=pcap_file,\n        max_packets=max_packets_needed,\n        memory_limit_mb=memory_limit_mb,\n        min_packet_size=min_packet_size,\n        payload_len=payload_len\n    )\n    \n    if not valid_packets:\n        print(f\"No valid packets found in {pcap_file}\")\n        return 0\n    \n    print(f\"Extracted {len(valid_packets)} valid packets from {pcap_file} in {time.time() - start_time:.1f} seconds\")\n    \n    # Shuffle and limit if needed\n    if len(valid_packets) > max_packets_needed:\n        random.shuffle(valid_packets)\n        packets_to_write = valid_packets[:max_packets_needed]\n        print(f\"Limiting to {len(packets_to_write)} packets for writing\")\n    else:\n        packets_to_write = valid_packets\n    \n    # Determine output directory\n    base_filename = os.path.splitext(os.path.basename(pcap_file))[0]\n    label_output_dir = os.path.join(output_dir, label_name)\n    \n    # Write packet objects to PCAP files\n    written_count = write_packets_in_chunks(\n        valid_packets=packets_to_write,\n        output_dir=label_output_dir,\n        base_filename=base_filename,\n        chunk_size=100\n    )\n    \n    # Clean up memory\n    valid_packets.clear()\n    packets_to_write.clear()\n    gc.collect()\n    \n    print(f\"Successfully wrote {written_count} packets for label '{label_name}' from {os.path.basename(pcap_file)}\")\n    return written_count\n\ndef process_regular_pcap_packet_level(pcap_file, output_dir, label_name, \n                                     max_packets_needed, min_packet_size=80, payload_len=128):\n    \"\"\"\n    Process a regular-sized PCAP file for packet-level analysis.\n    \n    Args:\n        pcap_file (str): Path to input PCAP file\n        output_dir (str): Base output directory\n        label_name (str): Label for the PCAP file\n        max_packets_needed (int): Maximum packets to extract\n        min_packet_size (int): Minimum packet size in bytes\n        payload_len (int): Payload length for processing (unused now but kept for compatibility)\n    \n    Returns:\n        int: Number of packets successfully processed\n    \"\"\"\n    print(f\"\\nðŸ“„ Processing regular file for packets: {os.path.basename(pcap_file)}\")\n    print(f\"Target packets from this file: {max_packets_needed}\")\n    \n    try:\n        # Load all packets\n        packets = rdpcap(pcap_file)\n        if len(packets) == 0:\n            print(f\"No packets found in {pcap_file}\")\n            return 0\n        \n        # Filter valid packets\n        valid_packets = []\n        for packet in packets:\n            if is_valid_packet(packet, min_packet_size):\n                valid_packets.append(packet.copy())  # Store actual packet objects\n        \n        print(f\"Found {len(valid_packets)} valid packets out of {len(packets)} total\")\n        \n        # Sample if needed\n        if len(valid_packets) > max_packets_needed:\n            print(f\"Sampling {max_packets_needed} packets from {len(valid_packets)} available\")\n            valid_packets = random.sample(valid_packets, max_packets_needed)\n        \n        # Write packets to PCAP files\n        base_filename = os.path.splitext(os.path.basename(pcap_file))[0]\n        label_output_dir = os.path.join(output_dir, label_name)\n        os.makedirs(label_output_dir, exist_ok=True)\n        \n        written_count = 0\n        for i, packet in enumerate(valid_packets):\n            packet_hash = hashlib.md5(f\"{pcap_file}_{i}_{time.time_ns()}_{random.randint(0, 10000)}\".encode()).hexdigest()[:8]\n            output_file = os.path.join(label_output_dir, f\"{base_filename}_packet_{i:04d}_{packet_hash}.pcap\")\n            \n            try:\n                # Write single packet to PCAP file\n                wrpcap(output_file, [packet])\n                written_count += 1\n            except Exception as e:\n                print(f\"Error writing packet {i}: {e}\")\n        \n        # Clean up memory\n        del packets\n        valid_packets.clear()\n        gc.collect()\n        \n        return written_count\n        \n    except Exception as e:\n        print(f\"Error processing {pcap_file}: {e}\")\n        return 0\n\ndef process_directory_packet_level(input_dir, output_dir, samples_per_label=5000,\n                                  min_packet_size=80, payload_len=128,\n                                  memory_limit_mb=800, large_file_threshold_mb=500):\n    \"\"\"\n    Process directory for packet-level analysis with large file handling.\n    \n    Args:\n        input_dir (str): Input directory with label subdirectories\n        output_dir (str): Output directory for processed packets\n        samples_per_label (int): Target number of packets per label\n        min_packet_size (int): Minimum packet size in bytes\n        payload_len (int): Payload length for processing\n        memory_limit_mb (int): Memory threshold for streaming\n        large_file_threshold_mb (int): File size threshold for streaming mode\n    \n    Returns:\n        dict: Results showing packets extracted per label\n    \"\"\"\n    total_processed = {}\n    \n    for label_name in os.listdir(input_dir):\n        label_path = os.path.join(input_dir, label_name)\n        if not os.path.isdir(label_path):\n            continue\n        \n        print(f\"\\nðŸ·ï¸ Processing label for packets: {label_name}\")\n        \n        # Collect PCAP files\n        pcap_files = []\n        for root, dirs, files in os.walk(label_path):\n            for file in files:\n                if file.lower().endswith(('.pcap', '.pcapng')):\n                    file_path = os.path.join(root, file)\n                    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n                    pcap_files.append((file_path, file_size_mb))\n        \n        if not pcap_files:\n            print(f\"No PCAP files found for {label_name}\")\n            continue\n        \n        # Sort by size (smaller first)\n        pcap_files.sort(key=lambda x: x[1])\n        \n        print(f\"Found {len(pcap_files)} PCAP files for {label_name}:\")\n        for file_path, size_mb in pcap_files:\n            print(f\"  ðŸ“ {os.path.basename(file_path)}: {size_mb:.1f}MB\")\n        \n        # Process files until target is met\n        extracted_count = 0\n        for file_path, file_size_mb in pcap_files:\n            if extracted_count >= samples_per_label:\n                print(f\"Reached target {samples_per_label} packets for {label_name}\")\n                break\n            \n            remaining_needed = samples_per_label - extracted_count\n            \n            if file_size_mb > large_file_threshold_mb:\n                print(f\"âš ï¸ Large file detected: {os.path.basename(file_path)} ({file_size_mb:.1f}MB). Using streaming mode.\")\n                packets_extracted = process_large_pcap_packet_level(\n                    pcap_file=file_path,\n                    output_dir=output_dir,\n                    label_name=label_name,\n                    max_packets_needed=remaining_needed,\n                    min_packet_size=min_packet_size,\n                    memory_limit_mb=memory_limit_mb,\n                    payload_len=payload_len\n                )\n            else:\n                print(f\"ðŸ“„ Processing regular file: {os.path.basename(file_path)} ({file_size_mb:.1f}MB)\")\n                packets_extracted = process_regular_pcap_packet_level(\n                    pcap_file=file_path,\n                    output_dir=output_dir,\n                    label_name=label_name,\n                    max_packets_needed=remaining_needed,\n                    min_packet_size=min_packet_size,\n                    payload_len=payload_len\n                )\n            \n            extracted_count += packets_extracted\n            print(f\"âœ… Progress for {label_name}: {extracted_count}/{samples_per_label} packets\")\n            \n            gc.collect()\n        \n        total_processed[label_name] = extracted_count\n        print(f\"ðŸŽ¯ Completed {label_name}: {extracted_count} packets extracted\")\n    \n    return total_processed\n\ndef process_pcaps_packet_level_kaggle(input_path, output_path, samples_per_label=5000):\n    \"\"\"\n    Main function for Kaggle packet-level processing.\n    \n    Args:\n        input_path (str): Input directory path\n        output_path (str): Output directory path\n        samples_per_label (int): Target packets per label\n    \n    Returns:\n        dict: Processing results\n    \"\"\"\n    print(\"ðŸš€ Starting packet-level PCAP processing optimized for Kaggle\")\n    print(f\"ðŸ“Š Memory available: {psutil.virtual_memory().available / (1024**3):.1f}GB\")\n    print(f\"ðŸŽ¯ Target packets per label: {samples_per_label}\")\n    \n    results = process_directory_packet_level(\n        input_dir=input_path,\n        output_dir=output_path,\n        samples_per_label=samples_per_label,\n        min_packet_size=80,\n        payload_len=128,\n        memory_limit_mb=800,\n        large_file_threshold_mb=300\n    )\n    \n    print(\"\\nðŸ“ˆ Final Results:\")\n    total_packets = 0\n    for label, count in results.items():\n        print(f\"  {label}: {count} packets\")\n        total_packets += count\n    \n    print(f\"ðŸŽ‰ Total packets extracted across all labels: {total_packets}\")\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T23:24:20.772253Z","iopub.execute_input":"2025-06-30T23:24:20.772621Z","iopub.status.idle":"2025-06-30T23:24:20.824274Z","shell.execute_reply.started":"2025-06-30T23:24:20.772595Z","shell.execute_reply":"2025-06-30T23:24:20.822236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## dataset.json creation","metadata":{}},{"cell_type":"code","source":"def filter_packets(packets):\n    \"\"\"\n    Filter out ARP, DHCP and other irrelevant packets according to research rules\n    \"\"\"\n    filtered_packets = []\n    \n    for packet in packets:\n        # Skip if packet is too small (< 80 bytes as mentioned in rules)\n        if len(packet) <= 80:\n            continue\n            \n        # Skip ARP packets (as per rules)\n        if packet.haslayer(ARP):\n            continue\n            \n        # Skip DHCP packets (as per rules)\n        if packet.haslayer(DHCP):\n            continue\n            \n        # Skip other irrelevant protocols\n        if packet.haslayer(DNS):\n            continue\n        if packet.haslayer(ICMP):\n            continue\n            \n        # Skip ICMPv6 packets (check by IPv6 layer and next header)\n        if packet.haslayer(IPv6):\n            # ICMPv6 has protocol number 58 in IPv6\n            if packet[IPv6].nh == 58:  # Next Header = ICMPv6\n                continue\n            \n        # Only keep TCP and UDP packets with IP layer\n        if packet.haslayer(IP) and (packet.haslayer(TCP) or packet.haslayer(UDP)):\n            filtered_packets.append(packet)\n        # Also accept IPv6 TCP/UDP packets\n        elif packet.haslayer(IPv6) and (packet.haslayer(TCP) or packet.haslayer(UDP)):\n            filtered_packets.append(packet)\n    \n    return filtered_packets\n\ndef extract_payload_only(packet):\n    \"\"\"\n    Extract only payload data, removing all headers as per research rules:\n    - Ethernet header\n    - IP header  \n    - TCP/UDP header and ports\n    \"\"\"\n    try:\n        if packet.haslayer(TCP):\n            # For TCP packets, get payload after TCP header\n            tcp_layer = packet[TCP]\n            payload = bytes(tcp_layer.payload)\n        elif packet.haslayer(UDP):\n            # For UDP packets, get payload after UDP header  \n            udp_layer = packet[UDP]\n            payload = bytes(udp_layer.payload)\n        else:\n            # Fallback: get payload after IP header (IPv4 or IPv6)\n            if packet.haslayer(IP):\n                ip_layer = packet[IP]\n                payload = bytes(ip_layer.payload)\n            elif packet.haslayer(IPv6):\n                ipv6_layer = packet[IPv6]\n                payload = bytes(ipv6_layer.payload)\n            else:\n                return \"\"\n            \n        # Convert to hex string\n        if len(payload) > 0:\n            hex_payload = binascii.hexlify(payload).decode()\n            return hex_payload\n        else:\n            return \"\"\n            \n    except Exception as e:\n        print(f\"Error extracting payload: {e}\")\n        return \"\"\n\ndef cut(obj, sec):\n    \"\"\"Cut object into sections\"\"\"\n    if not obj or len(obj) == 0:\n        return []\n        \n    result = [obj[i:i+sec] for i in range(0, len(obj), sec)]\n    \n    if not result or len(result) == 0:\n        return []\n        \n    remanent_count = len(result[0]) % 4\n    if remanent_count != 0:\n        result = [obj[i:i+sec+remanent_count] for i in range(0, len(obj), sec+remanent_count)]\n    \n    return result\n\ndef bigram_generation(packet_datagram, packet_len=64):\n    \"\"\"Generate bigrams from packet data\"\"\"\n    if not packet_datagram or len(packet_datagram) == 0:\n        return ''\n    \n    result = ''\n    generated_datagram = cut(packet_datagram, 1)\n    \n    if not generated_datagram or len(generated_datagram) == 0:\n        return ''\n    \n    token_count = 0\n    for sub_string_index in range(len(generated_datagram)):\n        if sub_string_index != (len(generated_datagram) - 1):\n            token_count += 1\n            if token_count > packet_len:\n                break\n            else:\n                merge_word_bigram = generated_datagram[sub_string_index] + generated_datagram[sub_string_index + 1]\n        else:\n            break\n        result += merge_word_bigram\n        result += ' '\n    \n    return result\n\ndef get_feature_packet_improved(pcap_file, payload_len=128):\n    \"\"\"\n    Extract packet-level features with proper filtering and header removal\n    Optimized for single packet per file (after splitcap)\n    \"\"\"\n    try:\n        packets = rdpcap(pcap_file)\n        \n        # Since each file contains one packet after splitcap\n        if not packets or len(packets) == 0:\n            return []\n            \n        # Filter the single packet\n        filtered_packets = filter_packets(packets)\n        \n        if not filtered_packets:\n            return []\n        \n        # Process the single valid packet\n        packet = filtered_packets[0]\n        payload_hex = extract_payload_only(packet)\n        \n        if not payload_hex:\n            return []\n        \n        # Generate bigrams from payload only\n        packet_data_string = bigram_generation(payload_hex, packet_len=payload_len)\n        \n        return [packet_data_string] if packet_data_string else []\n        \n    except Exception as e:\n        print(f\"Error extracting packet features from {pcap_file}: {e}\")\n        return []\n\ndef get_feature_flow_improved(pcap_file, payload_len=128, payload_pac=5):\n    \"\"\"\n    Extract flow-level features with proper filtering and header removal\n    Optimized for single flow per file (after splitcap)\n    \"\"\"\n    try:\n        packets = rdpcap(pcap_file)\n        \n        # Since each file contains one flow after splitcap\n        if not packets or len(packets) == 0:\n            return []\n            \n        # Filter all packets in the flow\n        filtered_packets = filter_packets(packets)\n        \n        # Need at least 3 packets for flow (even after filtering)\n        if len(filtered_packets) < 3:\n            return []\n        \n        # Limit to payload_packet count\n        packets_to_process = filtered_packets[:payload_pac]\n        \n        lengths = []\n        times = []\n        directions = []\n        flow_data_string = ''\n        \n        for i, packet in enumerate(packets_to_process):\n            payload_hex = extract_payload_only(packet)\n            \n            if payload_hex:\n                bigram_data = bigram_generation(payload_hex, packet_len=payload_len)\n                flow_data_string += bigram_data\n                \n                # Extract other flow features\n                lengths.append(len(payload_hex) // 2)  # Convert hex to byte length\n                times.append(float(packet.time))\n                directions.append(1 if i % 2 == 0 else -1)\n        \n        if not flow_data_string:\n            return []\n        \n        # Return: [payload, lengths, times, directions, message_types]\n        return [flow_data_string, lengths, times, directions, []]\n        \n    except Exception as e:\n        print(f\"Error extracting flow features from {pcap_file}: {e}\")\n        return []\n\ndef clean_single_pcap(source_file, target_file):\n    \"\"\"\n    Clean a single PCAP file by removing irrelevant packets\n    \"\"\"\n    try:\n        print(f\"Cleaning: {source_file}\")\n        packets = rdpcap(source_file)\n        filtered_packets = filter_packets(packets)\n        \n        if filtered_packets:\n            wrpcap(target_file, filtered_packets)\n            print(f\"Saved {len(filtered_packets)} packets (from {len(packets)}) to {target_file}\")\n            return len(filtered_packets)\n        else:\n            print(f\"No valid packets in {source_file}\")\n            return 0\n            \n    except Exception as e:\n        print(f\"Error processing {source_file}: {e}\")\n        return 0\n\ndef process_single_file(args):\n    \"\"\"\n    Process a single PCAP file - designed for parallel execution\n    Args: (pcap_file, dataset_level, payload_length, payload_packet)\n    Returns: (success, pcap_file, features)\n    \"\"\"\n    pcap_file, dataset_level, payload_length, payload_packet = args\n    \n    try:\n        if dataset_level == \"packet\":\n            features = get_feature_packet_improved(pcap_file, payload_len=payload_length)\n        else:\n            features = get_feature_flow_improved(pcap_file, payload_len=payload_length, payload_pac=payload_packet)\n        \n        if features and len(features) > 0:\n            return True, pcap_file, features\n        else:\n            return False, pcap_file, None\n            \n    except Exception as e:\n        print(f\"Error processing {pcap_file}: {e}\")\n        return False, pcap_file, None\n\ndef process_pcap_dataset(input_path, output_path, samples_per_label=1000, \n                        dataset_level=\"packet\", payload_length=128, payload_packet=5,\n                        max_workers=None,dataset_name=\"\"):\n    \"\"\"\n    Complete dataset processing function for Kaggle with parallel processing\n    \n    Args:\n        input_path: Path to input dataset with label folders\n        output_path: Path to save processed dataset\n        samples_per_label: Number of samples to extract per label\n        dataset_level: \"packet\" or \"flow\"\n        payload_length: Length for bigram generation\n        payload_packet: Number of packets per flow (for flow level)\n        max_workers: Number of parallel workers (None = auto-detect)\n    \n    Returns:\n        dataset: Dictionary containing processed dataset\n    \"\"\"\n    print(f\"Processing dataset: {input_path}\")\n    print(f\"Dataset level: {dataset_level}\")\n    print(f\"Samples per label: {samples_per_label}\")\n    \n    # Set max_workers if not specified\n    if max_workers is None:\n        max_workers = min(cpu_count(), 8)  # Don't use too many cores\n    print(f\"Using {max_workers} parallel workers\")\n    \n    # Create output directory\n    Path(output_path).mkdir(parents=True, exist_ok=True)\n    \n    # Get label folders\n    label_folders = [d for d in os.listdir(input_path) \n                    if os.path.isdir(os.path.join(input_path, d))]\n    \n    print(f\"Found labels: {label_folders}\")\n    \n    dataset = {}\n    label_id = {label: idx for idx, label in enumerate(label_folders)}\n    \n    processed_files = []\n    \n    for label_name in label_folders:\n        print(f\"\\n{'='*50}\")\n        print(f\"Processing label: {label_name}\")\n        print(f\"{'='*50}\")\n        \n        label_path = os.path.join(input_path, label_name)\n        \n        # Initialize dataset entry\n        label_idx = label_id[label_name]\n        if dataset_level == \"packet\":\n            dataset[label_idx] = {\"samples\": 0, \"payload\": {}}\n        else:  # flow\n            dataset[label_idx] = {\n                \"samples\": 0, \"payload\": {}, \"length\": {}, \n                \"time\": {}, \"direction\": {}, \"message_type\": {}\n            }\n        \n        # Collect all PCAP files (each contains one packet/flow after splitcap)\n        pcap_files = []\n        for root, dirs, files in os.walk(label_path):\n            for file in files:\n                if file.lower().endswith(('.pcap', '.pcapng')):\n                    pcap_files.append(os.path.join(root, file))\n        \n        if not pcap_files:\n            print(f\"No PCAP files found in {label_path}\")\n            continue\n        \n        print(f\"Found {len(pcap_files)} PCAP files\")\n        \n        # Sample files if needed (since each file = one sample after splitcap)\n        files_to_process = (random.sample(pcap_files, min(samples_per_label, len(pcap_files))) \n                          if len(pcap_files) > samples_per_label else pcap_files)\n        \n        print(f\"Processing {len(files_to_process)} files\")\n        \n        # Prepare arguments for parallel processing\n        args_list = [\n            (pcap_file, dataset_level, payload_length, payload_packet)\n            for pcap_file in files_to_process\n        ]\n        \n        # Process files in parallel\n        successful_extractions = 0\n        \n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all tasks\n            future_to_file = {\n                executor.submit(process_single_file, args): args[0] \n                for args in args_list\n            }\n            \n            # Process results as they complete\n            for future in tqdm.tqdm(as_completed(future_to_file), \n                                   total=len(future_to_file), \n                                   desc=f\"Processing {label_name}\"):\n                \n                success, pcap_file, features = future.result()\n                \n                if success and features:\n                    dataset[label_idx][\"samples\"] += 1\n                    sample_key = str(dataset[label_idx][\"samples\"])\n                    \n                    # Store features\n                    dataset[label_idx][\"payload\"][sample_key] = features[0]\n                    \n                    if dataset_level == \"flow\" and len(features) > 1:\n                        dataset[label_idx][\"length\"][sample_key] = features[1]\n                        dataset[label_idx][\"time\"][sample_key] = features[2]\n                        dataset[label_idx][\"direction\"][sample_key] = features[3]\n                        dataset[label_idx][\"message_type\"][sample_key] = features[4]\n                    \n                    processed_files.append(pcap_file)\n                    successful_extractions += 1\n        \n        print(f\"Label {label_name}: {dataset[label_idx]['samples']} samples extracted successfully\")\n        print(f\"Success rate: {successful_extractions}/{len(files_to_process)} ({100*successful_extractions/len(files_to_process):.1f}%)\")\n    \n    # Save results\n    dataset_file = os.path.join(output_path, f\"dataset_{dataset_name}.json\")\n    with open(dataset_file, \"w\") as f:\n        json.dump(dataset, f, ensure_ascii=False, indent=4)\n    \n    # Save processed files list\n    files_record = os.path.join(output_path, \"processed_files_ustf-tfc.txt\")\n    with open(files_record, \"w\") as f:\n        for file_path in processed_files:\n            f.write(f\"{file_path}\\n\")\n    \n    # Print statistics\n    total_samples = 0\n    print(f\"\\n{'='*50}\")\n    print(\"DATASET STATISTICS\")\n    print(f\"{'='*50}\")\n    for label_name in label_folders:\n        label_idx = label_id[label_name]\n        samples = dataset[label_idx][\"samples\"]\n        print(f\"{label_idx}\\t{label_name}\\t{samples}\")\n        total_samples += samples\n    print(f\"Total samples: {total_samples}\")\n    \n    return dataset\n\ndef clean_dataset_folder(input_path, output_path):\n    \"\"\"\n    Clean all PCAP files in a dataset folder structure\n    \"\"\"\n    print(f\"Cleaning dataset: {input_path} -> {output_path}\")\n    \n    cleaned_count = 0\n    total_count = 0\n    \n    for root, dirs, files in os.walk(input_path):\n        for file in files:\n            if file.lower().endswith(('.pcap', '.pcapng')):\n                total_count += 1\n                source_path = os.path.join(root, file)\n                \n                # Maintain folder structure in output\n                relative_path = os.path.relpath(root, input_path)\n                output_subfolder = os.path.join(output_path, relative_path)\n                Path(output_subfolder).mkdir(parents=True, exist_ok=True)\n                \n                # Create output filename\n                clean_filename = f\"clean_{file}\"\n                target_path = os.path.join(output_subfolder, clean_filename)\n                \n                # Clean the PCAP file\n                try:\n                    result = clean_single_pcap(source_path, target_path)\n                    if result > 0:\n                        cleaned_count += 1\n                except Exception as e:\n                    print(f\"Error cleaning {source_path}: {e}\")\n    \n    print(f\"Cleaning complete: {cleaned_count}/{total_count} files successfully cleaned\")\n    return output_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T23:24:23.052137Z","iopub.execute_input":"2025-06-30T23:24:23.05249Z","iopub.status.idle":"2025-06-30T23:24:23.092841Z","shell.execute_reply.started":"2025-06-30T23:24:23.052466Z","shell.execute_reply":"2025-06-30T23:24:23.091755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## transform to tsv for flow dataset","metadata":{}},{"cell_type":"code","source":"def clean_packet_payload(packet_payload, target_length=74):\n    \"\"\"\n    Clean and standardize packet payload\n    - Remove extra whitespace\n    - Truncate or pad to target length\n    - Handle empty/invalid payloads\n    - Use 4-character hex groups (0000 format)\n    \"\"\"\n    if not packet_payload or packet_payload.strip() == '':\n        # Empty packet - pad with 0000 groups\n        return ' '.join(['0000'] * target_length)\n    \n    # Split into tokens and clean\n    tokens = packet_payload.strip().split()\n    \n    # Filter and normalize tokens to 4-character hex groups\n    clean_tokens = []\n    for token in tokens:\n        if token == '<PAD>':\n            clean_tokens.append('0000')  # Replace <PAD> with 0000\n        elif len(token) == 4 and all(c in '0123456789abcdefABCDEF' for c in token):\n            clean_tokens.append(token.lower())  # Normalize to lowercase\n        elif len(token) == 2 and all(c in '0123456789abcdefABCDEF' for c in token):\n            # Convert 2-char hex to 4-char by padding with zeros\n            clean_tokens.append('00' + token.lower())\n        elif token in ['00', '0']:\n            clean_tokens.append('0000')  # Normalize short zeros\n        else:\n            # Skip invalid tokens or try to fix them\n            if len(token) > 4:\n                # Take first 4 characters if too long\n                clean_tokens.append(token[:4].lower())\n            else:\n                # Pad with zeros if too short\n                padded = token.ljust(4, '0').lower()\n                clean_tokens.append(padded)\n    \n    # Truncate or pad to target length\n    if len(clean_tokens) >= target_length:\n        return ' '.join(clean_tokens[:target_length])\n    else:\n        # Pad with 0000 groups\n        padding_needed = target_length - len(clean_tokens)\n        padded_tokens = clean_tokens + ['0000'] * padding_needed\n        return ' '.join(padded_tokens)\n\ndef process_flow_to_fixed_format(flow_payload, target_packets=5, packet_length=74):\n    \"\"\"\n    Process flow payload to have exactly 5 packets of length 74 each\n    Using 4-character hex groups (0000 format)\n    \"\"\"\n    if not flow_payload or flow_payload.strip() == '':\n        # Empty flow - create 5 empty packets with 0000 padding\n        empty_packet = ' '.join(['0000'] * packet_length)\n        return ' '.join([empty_packet] * target_packets)\n    \n    # Try to split into packets (assuming packets are separated by multiple spaces or newlines)\n    # First, normalize separators\n    normalized_payload = flow_payload.replace('\\n', ' ').replace('\\t', ' ')\n    \n    # Split into tokens\n    all_tokens = normalized_payload.split()\n    \n    # Clean and normalize tokens to 4-character hex groups\n    clean_tokens = []\n    for token in all_tokens:\n        if token == '<PAD>':\n            clean_tokens.append('0000')\n        elif len(token) == 4 and all(c in '0123456789abcdefABCDEF' for c in token):\n            clean_tokens.append(token.lower())\n        elif len(token) == 2 and all(c in '0123456789abcdefABCDEF' for c in token):\n            # Convert 2-char to 4-char hex\n            clean_tokens.append('00' + token.lower())\n        elif token in ['00', '0']:\n            clean_tokens.append('0000')\n        else:\n            # Try to fix other formats\n            if len(token) > 4:\n                clean_tokens.append(token[:4].lower())\n            elif len(token) < 4 and all(c in '0123456789abcdefABCDEF' for c in token):\n                clean_tokens.append(token.ljust(4, '0').lower())\n            # Skip completely invalid tokens\n    \n    # Create packets\n    packets = []\n    tokens_per_packet = packet_length\n    \n    for i in range(target_packets):\n        start_idx = i * tokens_per_packet\n        end_idx = start_idx + tokens_per_packet\n        \n        if start_idx < len(clean_tokens):\n            packet_tokens = clean_tokens[start_idx:end_idx]\n            \n            # Pad packet if needed with 0000\n            if len(packet_tokens) < tokens_per_packet:\n                padding_needed = tokens_per_packet - len(packet_tokens)\n                packet_tokens.extend(['0000'] * padding_needed)\n                \n            packets.append(' '.join(packet_tokens))\n        else:\n            # No more tokens - create empty packet with 0000 padding\n            empty_packet = ' '.join(['0000'] * tokens_per_packet)\n            packets.append(empty_packet)\n    \n    # Join packets with a separator (e.g., ' | ')\n    return ' | '.join(packets)\n\ndef analyze_dataset_structure(dataset_path):\n    \"\"\"\n    Analyze the current dataset structure to understand the data\n    \"\"\"\n    print(\"Analyzing dataset structure...\")\n    \n    with open(dataset_path, 'r', encoding='utf-8') as f:\n        dataset = json.load(f)\n    \n    stats = {\n        'total_labels': len(dataset),\n        'samples_per_label': {},\n        'payload_length_stats': [],\n        'common_tokens': Counter(),\n        'empty_payloads': 0\n    }\n    \n    for label_id, label_data in dataset.items():\n        samples = label_data.get('samples', 0)\n        stats['samples_per_label'][label_id] = samples\n        \n        # Analyze payloads\n        for sample_id, payload in label_data.get('payload', {}).items():\n            if not payload or payload.strip() == '':\n                stats['empty_payloads'] += 1\n                continue\n                \n            tokens = payload.split()\n            stats['payload_length_stats'].append(len(tokens))\n            \n            # Count common tokens (sample first 1000 for speed)\n            for token in tokens[:100]:\n                stats['common_tokens'][token] += 1\n    \n    # Print analysis\n    print(f\"Dataset Analysis:\")\n    print(f\"  Total labels: {stats['total_labels']}\")\n    print(f\"  Samples per label: {dict(stats['samples_per_label'])}\")\n    print(f\"  Empty payloads: {stats['empty_payloads']}\")\n    \n    if stats['payload_length_stats']:\n        print(f\"  Payload length stats:\")\n        print(f\"    Min: {min(stats['payload_length_stats'])} tokens\")\n        print(f\"    Max: {max(stats['payload_length_stats'])} tokens\") \n        print(f\"    Mean: {np.mean(stats['payload_length_stats']):.1f} tokens\")\n        print(f\"    Median: {np.median(stats['payload_length_stats']):.1f} tokens\")\n    \n    print(f\"  Most common tokens: {stats['common_tokens'].most_common(10)}\")\n    \n    # Check if data is in 4-character hex format\n    hex4_count = sum(1 for token, count in stats['common_tokens'].items() if len(token) == 4)\n    hex2_count = sum(1 for token, count in stats['common_tokens'].items() if len(token) == 2)\n    \n    print(f\"  Token format analysis:\")\n    print(f\"    4-character hex tokens: {hex4_count}\")\n    print(f\"    2-character hex tokens: {hex2_count}\")\n    print(f\"    Detected format: {'4-char hex groups (0000)' if hex4_count > hex2_count else '2-char hex pairs (00)'}\")\n    \n    return stats\n\ndef polish_dataset(input_dataset_path, output_tsv_path, target_packets=5, packet_length=74):\n    \"\"\"\n    Polish the dataset and create final TSV file\n    \"\"\"\n    print(f\"Polishing dataset...\")\n    print(f\"Target format: {target_packets} packets Ã— {packet_length} tokens each\")\n    \n    # Load dataset\n    with open(input_dataset_path, 'r', encoding='utf-8') as f:\n        dataset = json.load(f)\n    \n    # Prepare TSV data\n    tsv_data = []\n    total_samples = 0\n    successful_samples = 0\n    \n    # Process each label\n    for label_id, label_data in tqdm.tqdm(dataset.items(), desc=\"Processing labels\"):\n        samples = label_data.get('samples', 0)\n        \n        # Process each sample\n        for sample_id, payload in label_data.get('payload', {}).items():\n            total_samples += 1\n            \n            try:\n                # Process payload to fixed format\n                processed_payload = process_flow_to_fixed_format(\n                    payload, target_packets=target_packets, packet_length=packet_length\n                )\n                \n                # Add to TSV data\n                tsv_data.append([label_id, processed_payload])\n                successful_samples += 1\n                \n            except Exception as e:\n                print(f\"Error processing sample {sample_id} from label {label_id}: {e}\")\n                continue\n    \n    # Verify consistency\n    print(f\"Verifying consistency...\")\n    payload_lengths = [len(row[1].split()) for row in tsv_data]\n    unique_lengths = set(payload_lengths)\n    \n    expected_length = target_packets * (packet_length + 1) + (target_packets - 1)  # +1 for spaces, separators\n    \n    print(f\"Payload token counts: {unique_lengths}\")\n    print(f\"Expected approximate length: ~{target_packets * packet_length} tokens\")\n    \n    # Write TSV file\n    print(f\"Writing TSV file to {output_tsv_path}...\")\n    with open(output_tsv_path, 'w', newline='', encoding='utf-8') as f:\n        tsv_writer = csv.writer(f, delimiter='\\t')\n        \n        # Write header\n        tsv_writer.writerow(['label', 'text_a'])\n        \n        # Write data\n        for row in tsv_data:\n            tsv_writer.writerow(row)\n    \n    print(f\"Dataset polishing completed!\")\n    print(f\"  Total samples processed: {total_samples}\")\n    print(f\"  Successful samples: {successful_samples}\")\n    print(f\"  Output file: {output_tsv_path}\")\n    \n    return successful_samples\n\ndef validate_final_dataset(tsv_path, sample_size=10):\n    \"\"\"\n    Validate the final dataset and show samples\n    \"\"\"\n    print(f\"Validating final dataset...\")\n    \n    # Read TSV\n    with open(tsv_path, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f, delimiter='\\t')\n        rows = list(reader)\n    \n    if not rows:\n        print(\"No data found in TSV file!\")\n        return\n    \n    # Check consistency\n    payload_lengths = []\n    label_counts = Counter()\n    \n    for row in rows:\n        payload = row['text_a']\n        label = row['label']\n        \n        # Count tokens (packets are separated by ' | ')\n        packets = payload.split(' | ')\n        total_tokens = sum(len(packet.split()) for packet in packets)\n        payload_lengths.append(total_tokens)\n        \n        label_counts[label] += 1\n    \n    # Print validation results\n    print(f\"Validation Results:\")\n    print(f\"  Total samples: {len(rows)}\")\n    print(f\"  Labels distribution: {dict(label_counts)}\")\n    print(f\"  Payload length consistency:\")\n    \n    unique_lengths = set(payload_lengths)\n    if len(unique_lengths) == 1:\n        print(f\"    âœ“ All payloads have consistent length: {list(unique_lengths)[0]} tokens\")\n    else:\n        print(f\"    âš  Payload lengths vary: {unique_lengths}\")\n        print(f\"    Min: {min(payload_lengths)}, Max: {max(payload_lengths)}\")\n    \n    # Show samples\n    print(f\"\\nSample entries:\")\n    for i, row in enumerate(rows[:sample_size]):\n        payload = row['text_a']\n        label = row['label']\n        packets = payload.split(' | ')\n        \n        print(f\"  Sample {i+1} (Label {label}):\")\n        print(f\"    Packets: {len(packets)}\")\n        for j, packet in enumerate(packets):\n            tokens = packet.split()\n            print(f\"    Packet {j+1}: {len(tokens)} tokens - {' '.join(tokens[:10])}{'...' if len(tokens) > 10 else ''}\")\n        print()\n\n# Main execution\ndef main():\n    # Configuration\n    if not os.path.exists(\"/kaggle/working/tsv_file\"):\n        os.makedirs(\"tsv_file\")\n    input_dataset_path = \"dataset.json\"  # Path to your generated dataset\n    output_tsv_path = \"/kaggle/working/tsv_file/dataset_cicio.tsv\"  # Output TSV file\n    \n    target_packets = 5      # Exactly 5 packets per flow\n    packet_length = 74      # Exactly 74 tokens per packet\n    \n    print(\"=\"*60)\n    print(\"DATASET POLISHING FOR BERT FINE-TUNING\")\n    print(\"=\"*60)\n    \n    # Step 1: Analyze current dataset\n    if os.path.exists(input_dataset_path):\n        stats = analyze_dataset_structure(input_dataset_path)\n    else:\n        print(f\"Error: Dataset file {input_dataset_path} not found!\")\n        return\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    # Step 2: Polish and create TSV using correct packet-based approach\n    successful_samples = polish_dataset(\n        input_dataset_path=input_dataset_path,\n        output_tsv_path=output_tsv_path,\n        target_packets=target_packets,\n        packet_length=packet_length\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    # Step 3: Validate final result\n    if os.path.exists(output_tsv_path):\n        validate_final_dataset(output_tsv_path, sample_size=5)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"POLISHING COMPLETED!\")\n    print(\"=\"*60)\n    print(f\"Your dataset is now ready for BERT fine-tuning: {output_tsv_path}\")\n    print(f\"Format: {target_packets} packets Ã— {packet_length} hex groups (4-char each)\")\n    print(f\"Padding: 0000 (4-character hex groups)\")\n    print(f\"All samples have consistent length for efficient training!\")\n    print(f\"Example format: 000c 0c62 62c3 c3ce ... | 1234 5678 9abc def0 ... | ...\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## applied function","metadata":{}},{"cell_type":"markdown","source":"### Split pcap packet level","metadata":{}},{"cell_type":"code","source":"result_packet = process_pcaps_packet_level_kaggle(\n        input_path=\"/kaggle/input/pcap-data/Tor/Tor\",\n        output_path=\"/kaggle/working/tor\", \n        samples_per_label=5000\n    )\nprint(\"Processing complete!\")\nprint(f\"Results: {results}\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_packet = process_pcaps_packet_level_kaggle(\n        input_path=\"/kaggle/input/pcap-data/CICIot2022/CICIot2022\",\n        output_path=\"/kaggle/working/packet_level_split/CICIot2022\", \n        samples_per_label=5000\n    )\nprint(\"Processing complete!\")\nprint(f\"Results: {results}\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_packet = process_pcaps_packet_level_kaggle(\n        input_path=\"/kaggle/working/ISCX_VPN_pcap\",\n        output_path=\"/kaggle/working/packet_level_split/ISCX_VPN\", \n        samples_per_label=5000\n    )\nprint(\"Processing complete!\")\nprint(f\"Results: {results}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_packet = process_pcaps_packet_level_kaggle(\n        input_path=\"/kaggle/working/organized_ustf_tfc\",\n        output_path=\"/kaggle/working/packet_level_split/ustf-tfc\", \n        samples_per_label=5000\n    )\nprint(\"Processing complete!\")\nprint(f\"Results: {results}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T16:06:14.955052Z","iopub.execute_input":"2025-06-16T16:06:14.955405Z","iopub.status.idle":"2025-06-16T16:40:41.960343Z","shell.execute_reply.started":"2025-06-16T16:06:14.955372Z","shell.execute_reply":"2025-06-16T16:40:41.959471Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir \"/kaggle/working/packet_level_split/ustf-tfc\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T16:05:56.749981Z","iopub.execute_input":"2025-06-16T16:05:56.750471Z","iopub.status.idle":"2025-06-16T16:05:56.957005Z","shell.execute_reply.started":"2025-06-16T16:05:56.75044Z","shell.execute_reply":"2025-06-16T16:05:56.955455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generation of dataset json file","metadata":{}},{"cell_type":"code","source":"# generated datasetjson file \ndataset = process_pcap_dataset(\n    input_path=\"/kaggle/working/tor\",\n    output_path=\"/kaggle/working/dataset\",\n    samples_per_label=5000,\n    dataset_level=\"packet\",\n    payload_length=128,\n    payload_packet=5,\n    max_workers=4  # Custom worker count\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generated datasetjson file \ndataset = process_pcap_dataset(\n    input_path=\"/kaggle/working/packet_level_split/CICIot2022\",\n    output_path=\"/kaggle/working/dataset\",\n    dataset_name=\"ciciot_packet\",\n    samples_per_label=5000,\n    dataset_level=\"packet\",\n    payload_length=128,\n    payload_packet=5,\n    max_workers=4  # Custom worker count\n)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generated datasetjson file \ndataset = process_pcap_dataset(\n    input_path=\"/kaggle/working/packet_level_split/ISCX_VPN\",\n    output_path=\"/kaggle/working/dataset\",\n    dataset_name=\"iscxVpn_packet\",\n    samples_per_label=5000,\n    dataset_level=\"packet\",\n    payload_length=128,\n    payload_packet=5,\n    max_workers=4  # Custom worker count\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generated datasetjson file \ndataset = process_pcap_dataset(\n    input_path=\"/kaggle/working/packet_level_split/ustf-tfc\",\n    output_path=\"/kaggle/working/dataset\",\n    dataset_name=\"ustf-tfc_packet\",\n    samples_per_label=5000,\n    dataset_level=\"packet\",\n    payload_length=128,\n    payload_packet=5,\n    max_workers=4  # Custom worker count\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T18:04:54.298056Z","iopub.execute_input":"2025-06-16T18:04:54.298998Z","iopub.status.idle":"2025-06-16T18:05:55.720519Z","shell.execute_reply.started":"2025-06-16T18:04:54.298963Z","shell.execute_reply":"2025-06-16T18:05:55.719418Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### generation of tsv file for packet level ","metadata":{}},{"cell_type":"code","source":"# generation tsv file fo json dataset\nlabels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\"]\n\ndef normalize_payload(payload, length=640):\n    \"\"\"Ensure payload is exactly `length` characters by padding or trimming.\"\"\"\n    pad_unit = \" 0000\"\n    while len(payload) < length:\n        payload += pad_unit\n    return payload[:length]  # trim if it exceeds\n\n# Load dataset\nwith open(\"/kaggle/working/dataset/dataset_ustf-tfc_packet.json\", \"r\") as file:\n    dataset = json.load(file)\n\n# Write TSV\nwith open(\"/kaggle/working/tsv_file/dataset_ustf-tfc_packet.tsv\", \"w\", newline='') as tsvfile:\n    writer = csv.writer(tsvfile, delimiter='\\t')\n    writer.writerow([\"label\", \"text_a\"])  # Header\n\n    for label in labels:\n        # Get payloads from JSON\n        payloads = list(dataset[label][\"payload\"].values())\n\n        for payload in payloads:\n            normalized = normalize_payload(payload)\n            writer.writerow([label, normalized])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T18:05:55.722317Z","iopub.execute_input":"2025-06-16T18:05:55.722631Z","iopub.status.idle":"2025-06-16T18:05:58.262152Z","shell.execute_reply.started":"2025-06-16T18:05:55.722603Z","shell.execute_reply":"2025-06-16T18:05:58.261129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#generation tsv files for npy format dataset\n\n# Load .npy files\ndataset_save_path = \"/kaggle/input/etbert-dataset/USTC-TFC_dataset/\"\n\nx_payload_train = np.load(dataset_save_path + \"x_payload_train.npy\", allow_pickle=True)\nx_payload_test = np.load(dataset_save_path + \"x_payload_test.npy\", allow_pickle=True)\nx_payload_valid = np.load(dataset_save_path + \"x_payload_valid.npy\", allow_pickle=True)\n\ny_train = np.load(dataset_save_path + \"y_train.npy\", allow_pickle=True)\ny_test = np.load(dataset_save_path + \"y_test.npy\", allow_pickle=True)\ny_valid = np.load(dataset_save_path + \"y_valid.npy\", allow_pickle=True)\n\n# Merge all data\nx_all = np.concatenate([x_payload_train, x_payload_test, x_payload_valid], axis=0)\ny_all = np.concatenate([y_train, y_test, y_valid], axis=0)\n\n# Write to TSV\noutput_tsv_path = \"/kaggle/working/tsv_file/dataset_tfc_packet_l.tsv\"\nwith open(output_tsv_path, \"w\", newline='') as tsvfile:\n    writer = csv.writer(tsvfile, delimiter='\\t')\n    writer.writerow([\"label\", \"text_a\"])  # Header\n\n    for payload, label in zip(x_all, y_all):\n        writer.writerow([str(label), payload])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T16:01:43.304957Z","iopub.execute_input":"2025-07-07T16:01:43.305325Z","iopub.status.idle":"2025-07-07T16:01:44.876963Z","shell.execute_reply.started":"2025-07-07T16:01:43.305301Z","shell.execute_reply":"2025-07-07T16:01:44.87618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Other","metadata":{}},{"cell_type":"code","source":"# Organize ISXVPN dataset pcap\nresults = organize_kaggle_pcaps()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Organize ustf-tfc dataset pcap\nresults = organize_ustf_tfc_kaggle()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# finetuning etBert","metadata":{}},{"cell_type":"markdown","source":"## etBert","metadata":{}},{"cell_type":"code","source":"#fine_tuning the pretrained_model for ustf-tfc\n!python3 /kaggle/input/et-bert/run_classifier_copy.py --pretrained_model_path /kaggle/working/finetuned_models/finetuned_model_ustf-tfc_l1.bin  \\\n                                   --vocab_path /kaggle/input/etbert-model/ET-BERT/models/encryptd_vocab.txt \\\n                                   --config_path /kaggle/input/etbert-model/ET-BERT/models/bert_base_config.json\\\n                                   --output_model_path /kaggle/working/finetuned_models/finetuned_model_ustf-tfc_l.bin\\\n                                   --output_path_confusion_matrix  /kaggle/working/confusion_matrix/conf_mat_ustf-tfc_l.csv\\\n                                    --output_result /kaggle/working/result_json/result_ustf-tfc_l.json\\\n                                   --train_path /kaggle/working/tsv_file_split/dataset_tfc_packet_l_train.tsv\\\n                                   --dev_path /kaggle/working/tsv_file_split/dataset_tfc_packet_l_test.tsv\\\n                                   --test_path /kaggle/working/tsv_file_split/dataset_tfc_packet_l_test.tsv\\\n                                    --test_only true \\\n                                   --epochs_num 10 --batch_size 32 --embedding word_pos_seg \\\n                                   --encoder transformer --mask fully_visible \\\n                                   --seq_length 128 --learning_rate 2e-5\n                                   ","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-07-08T11:01:01.859551Z","iopub.execute_input":"2025-07-08T11:01:01.859846Z","iopub.status.idle":"2025-07-08T11:03:47.101701Z","shell.execute_reply.started":"2025-07-08T11:01:01.859816Z","shell.execute_reply":"2025-07-08T11:03:47.100782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:58:18.934664Z","iopub.execute_input":"2025-07-08T10:58:18.934951Z","iopub.status.idle":"2025-07-08T10:58:19.055221Z","shell.execute_reply.started":"2025-07-08T10:58:18.934922Z","shell.execute_reply":"2025-07-08T10:58:19.054381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fine_tuning the pretrained_model for ustf-tfc\n!python3  /kaggle/input/et-bert/run_classifier_copy.py --pretrained_model_path /kaggle/working/finetuned_models/finetuned_model_ustf-tfc_l1.bin \\\n                                   --vocab_path /kaggle/input/etbert-model/ET-BERT/models/encryptd_vocab.txt \\\n                                   --config_path /kaggle/input/etbert-model/ET-BERT/models/bert_base_config.json\\\n                                   --output_model_path /kaggle/working/finetuned_models/finetuned_model_ustf-tfc2.bin\\\n                                   --output_path_confusion_matrix  /kaggle/working/confusion_matrix/conf_mat_ustf-tfc1.csv\\\n                                   --train_path /kaggle/working/tsv_file_split/dataset_ustf-tfc_packet_train.tsv\\\n                                   --dev_path /kaggle/working/tsv_file_split/dataset_ustf-tfc_packet_test.tsv\\\n                                   --test_path /kaggle/working/tsv_file_split/dataset_ustf-tfc_packet_test.tsv\\\n                                   --epochs_num 6 --batch_size 32 --embedding word_pos_seg \\\n                                   --encoder transformer --mask fully_visible \\\n                                   --seq_length 128 --learning_rate 2e-5\n                                   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T22:26:31.454183Z","iopub.execute_input":"2025-06-28T22:26:31.454449Z","iopub.status.idle":"2025-06-29T01:03:51.584629Z","shell.execute_reply.started":"2025-06-28T22:26:31.454425Z","shell.execute_reply":"2025-06-29T01:03:51.583975Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir finetuned_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T16:32:29.33116Z","iopub.execute_input":"2025-07-07T16:32:29.33198Z","iopub.status.idle":"2025-07-07T16:32:29.490944Z","shell.execute_reply.started":"2025-07-07T16:32:29.331924Z","shell.execute_reply":"2025-07-07T16:32:29.489475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fine_tuning the pretrained_model for isxTor\n!python3  /kaggle/input/et-bert/run_classifier_copy.py --pretrained_model_path /kaggle/working/finetuned_models/finetuned_model_isxtor2.bin \\\n                                   --vocab_path /kaggle/input/etbert-model/ET-BERT/models/encryptd_vocab.txt \\\n                                   --config_path /kaggle/input/etbert-model/ET-BERT/models/bert_base_config.json\\\n                                   --output_model_path /kaggle/working/finetuned_models/finetuned_model_isxtor3.bin\\\n                                   --output_path_confusion_matrix  /kaggle/working/confusion_matrix/conf_mat_isxtor2.csv\\\n                                  --output_result /kaggle/working/result_json/result_isxtor.json\\\n                                   --train_path /kaggle/working/tsv_file_split/dataset_tor_packet_train.tsv\\\n                                   --dev_path /kaggle/working/tsv_file_split/dataset_tor_packet_test.tsv\\\n                                   --test_path /kaggle/working/tsv_file_split/dataset_tor_packet_test.tsv\\\n                                   --epochs_num 10 --batch_size 32 --embedding word_pos_seg \\\n                                   --encoder transformer --mask fully_visible \\\n                                   --seq_length 128 --learning_rate 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T18:04:31.37818Z","iopub.execute_input":"2025-06-29T18:04:31.378561Z","iopub.status.idle":"2025-06-29T19:47:27.643214Z","shell.execute_reply.started":"2025-06-29T18:04:31.378536Z","shell.execute_reply":"2025-06-29T19:47:27.642404Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fine_tuning the pretrained_model for isxTor\n!python3  /kaggle/input/et-bert/run_classifier_copy.py --pretrained_model_path /kaggle/input/pretrained-model/pretrain_model/etBert_pretrained_model.bin\\\n                                   --vocab_path /kaggle/input/etbert-model/ET-BERT/models/encryptd_vocab.txt \\\n                                   --config_path /kaggle/input/etbert-model/ET-BERT/models/bert_base_config.json\\\n                                   --output_model_path /kaggle/working/finetuned_models/finetuned_model_isxtor1.bin\\\n                                   --output_path_confusion_matrix  /kaggle/working/confusion_matrix/conf_mat_isxtor.csv\\\n                                  --output_result /kaggle/working/result_json/result_isxtor.json\\\n                                   --train_path /kaggle/working/tsv_file_split/dataset_tor_packet_train.tsv\\\n                                   --dev_path /kaggle/working/tsv_file_split/dataset_tor_packet_test.tsv\\\n                                   --test_path /kaggle/working/tsv_file_split/dataset_tor_packet_test.tsv\\\n                                   --epochs_num 5 --batch_size 16 --embedding word_pos_seg \\\n                                   --encoder transformer --mask fully_visible \\\n                                   --seq_length 512 --learning_rate 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T01:10:07.184814Z","iopub.execute_input":"2025-07-01T01:10:07.18551Z","iopub.status.idle":"2025-07-01T04:21:43.053982Z","shell.execute_reply.started":"2025-07-01T01:10:07.185477Z","shell.execute_reply":"2025-07-01T04:21:43.053106Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fine_tuning the pretrained_model for isxTor\n!python3  /kaggle/input/et-bert/run_classifier_copy.py --pretrained_model_path /kaggle/working/finetuned_models/finetuned_model_isxtor3.bin \\\n                                   --vocab_path /kaggle/input/etbert-model/ET-BERT/models/encryptd_vocab.txt \\\n                                   --config_path /kaggle/input/etbert-model/ET-BERT/models/bert_base_config.json\\\n                                   --output_model_path /kaggle/working/finetuned_models/finetuned_model_isxtor4.bin\\\n                                   --output_path_confusion_matrix  /kaggle/working/confusion_matrix/conf_mat_isxtor4.csv\\\n                                  --output_result /kaggle/working/result_json/result_isxtor4.json\\\n                                   --train_path /kaggle/working/tsv_file_split/dataset_tor_packet_train.tsv\\\n                                   --dev_path /kaggle/working/tsv_file_split/dataset_tor_packet_test.tsv\\\n                                   --test_path /kaggle/working/tsv_file_split/dataset_tor_packet_test.tsv\\\n                                   --epochs_num 15 --batch_size 32 --embedding word_pos_seg \\\n                                   --encoder transformer --mask fully_visible \\\n                                   --seq_length 128 --learning_rate 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T19:47:27.644742Z","iopub.execute_input":"2025-06-29T19:47:27.644971Z","iopub.status.idle":"2025-06-29T22:20:37.12965Z","shell.execute_reply.started":"2025-06-29T19:47:27.644943Z","shell.execute_reply":"2025-06-29T22:20:37.12885Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#fine_tuning the pretrained_model for isxvpn\n!python3  /kaggle/input/et-bert/run_classifier_copy.py --pretrained_model_path /kaggle/input/pretrained-model/pretrain_model/etBert_pretrained_model.bin\\\n                                   --vocab_path /kaggle/input/etbert-model/ET-BERT/models/encryptd_vocab.txt \\\n                                   --config_path /kaggle/input/etbert-model/ET-BERT/models/bert_base_config.json\\\n                                   --output_model_path /kaggle/working/finetuned_models/finetuned_model_iscxvpn.bin\\\n                                   --output_path_confusion_matrix  /kaggle/working/confusion_matrix/conf_mat_iscxvpn.csv\\\n                                  --output_result /kaggle/working/result_json/result_iscxvpn.json\\\n                                   --train_path /kaggle/working/tsv_file_split/dataset_iscxvpn_packet_train.tsv\\\n                                   --dev_path /kaggle/working/tsv_file_split/dataset_iscxvpn_packet_test.tsv\\\n                                   --test_path /kaggle/working/tsv_file_split/dataset_iscxvpn_packet_test.tsv\\\n                                   --epochs_num 15 --batch_size 32 --embedding word_pos_seg \\\n                                   --encoder transformer --mask fully_visible \\\n                                   --seq_length 128 --learning_rate 2e-5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T22:55:57.113976Z","iopub.execute_input":"2025-06-29T22:55:57.114646Z","iopub.status.idle":"2025-06-30T01:04:48.685635Z","shell.execute_reply.started":"2025-06-29T22:55:57.114619Z","shell.execute_reply":"2025-06-30T01:04:48.684415Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:29:10.644962Z","iopub.execute_input":"2025-09-27T10:29:10.64563Z","iopub.status.idle":"2025-09-27T10:29:10.908899Z","shell.execute_reply.started":"2025-09-27T10:29:10.645587Z","shell.execute_reply":"2025-09-27T10:29:10.908159Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df5=pd.read_csv(\"/kaggle/working/tsv_file_split/dataset_iscxvpn_packet_train.tsv\", sep=\"\\t\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_Service_dataset/x_payload_test.npy\")\nX_train=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_Service_dataset/x_payload_train.npy\")\nX_valid=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_Service_dataset/x_payload_valid.npy\")\ny_train=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_Service_dataset/y_train.npy\")\ny_test=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_Service_dataset/y_test.npy\")\ny_valid=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_Service_dataset/y_valid.npy\")\ndata_train=pd.DataFrame({\"label\":y_train,\"text_a\":X_train})\ndata_test=pd.DataFrame({\"label\":y_test,\"text_a\":X_test})\ndata_valid=pd.DataFrame({\"label\":y_valid,\"text_a\":X_valid})\ndata=pd.concat([data_train,data_test,data_valid])\ndf1=data[data[\"label\"].isin([0,1,2,3,4,5])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:09:47.880324Z","iopub.execute_input":"2025-09-28T08:09:47.880631Z","iopub.status.idle":"2025-09-28T08:09:48.045953Z","shell.execute_reply.started":"2025-09-28T08:09:47.880611Z","shell.execute_reply":"2025-09-28T08:09:48.045088Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"X_test=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_app_dataset/x_payload_test.npy\")\nX_train=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_app_dataset/x_payload_train.npy\")\nX_valid=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_app_dataset/x_payload_valid.npy\")\ny_train=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_app_dataset/y_train.npy\")\ny_test=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_app_dataset/y_test.npy\")\ny_valid=np.load(\"/kaggle/input/etbert-dataset/ISCX-VPN_app_dataset/y_valid.npy\")\ndata_train=pd.DataFrame({\"label\":y_train,\"text_a\":X_train})\ndata_test=pd.DataFrame({\"label\":y_test,\"text_a\":X_test})\ndata_valid=pd.DataFrame({\"label\":y_valid,\"text_a\":X_valid})\ndata=pd.concat([data_train,data_test,data_valid])\ndf2=data[data[\"label\"]==10]\n\n#data.to_csv(\"/kaggle/working/tsv_file/dataset_iscxvpn_packet.tsv\",sep=\"\\t\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:09:50.281947Z","iopub.execute_input":"2025-09-28T08:09:50.282213Z","iopub.status.idle":"2025-09-28T08:09:50.416887Z","shell.execute_reply.started":"2025-09-28T08:09:50.282194Z","shell.execute_reply":"2025-09-28T08:09:50.416366Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"X_test=np.load(\"/kaggle/input/etbert-dataset/USTC-TFC_dataset/x_payload_test.npy\")\nX_train=np.load(\"/kaggle/input/etbert-dataset/USTC-TFC_dataset/x_payload_train.npy\")\nX_valid=np.load(\"/kaggle/input/etbert-dataset/USTC-TFC_dataset/x_payload_valid.npy\")\ny_train=np.load(\"/kaggle/input/etbert-dataset/USTC-TFC_dataset/y_train.npy\")\ny_test=np.load(\"/kaggle/input/etbert-dataset/USTC-TFC_dataset/y_test.npy\")\ny_valid=np.load(\"/kaggle/input/etbert-dataset/USTC-TFC_dataset/y_valid.npy\")\ndata_train=pd.DataFrame({\"label\":y_train,\"text_a\":X_train})\ndata_test=pd.DataFrame({\"label\":y_test,\"text_a\":X_test})\ndata_valid=pd.DataFrame({\"label\":y_valid,\"text_a\":X_valid})\ndata=pd.concat([data_train,data_test,data_valid])\ndata[\"label\"].value_counts()\ndata.to_csv(\"/kaggle/working/tsv_file/dataset_ustf-tfc_packet_l.tsv\",sep=\"\\t\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:38:10.065881Z","iopub.execute_input":"2025-09-27T10:38:10.066598Z","iopub.status.idle":"2025-09-27T10:38:11.080477Z","shell.execute_reply.started":"2025-09-27T10:38:10.066574Z","shell.execute_reply":"2025-09-27T10:38:11.079679Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"df=pd.concat([df1,df2])\ndf.loc[df[\"label\"]==10,\"label\"]=6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:09:53.196498Z","iopub.execute_input":"2025-09-28T08:09:53.196758Z","iopub.status.idle":"2025-09-28T08:09:53.207737Z","shell.execute_reply.started":"2025-09-28T08:09:53.19674Z","shell.execute_reply":"2025-09-28T08:09:53.206966Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"df.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:10:07.837788Z","iopub.execute_input":"2025-09-28T08:10:07.838585Z","iopub.status.idle":"2025-09-28T08:10:07.844027Z","shell.execute_reply.started":"2025-09-28T08:10:07.838553Z","shell.execute_reply":"2025-09-28T08:10:07.84347Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"label\n2    5000\n4    5000\n3    5000\n5    5000\n0    5000\n1    5000\n6    5000\nName: count, dtype: int64"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/tsv_file/dataset_iscxvpn_packet_l.tsv\",sep=\"\\t\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:10:47.065977Z","iopub.execute_input":"2025-09-28T08:10:47.066256Z","iopub.status.idle":"2025-09-28T08:10:47.3721Z","shell.execute_reply.started":"2025-09-28T08:10:47.066236Z","shell.execute_reply":"2025-09-28T08:10:47.371552Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the TSV file\ndata = pd.read_csv('/kaggle/working/tsv_file/dataset_tfc_packet_l.tsv', sep='\\t')\nprint(len(data))\n\n# Split into features (X) and labels (y)\nX = data['text_a']\ny = data['label']\n\n# Perform the train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Optionally combine into DataFrames\ntrain_df = pd.DataFrame({'label': y_train, 'text_a': X_train})\ntest_df = pd.DataFrame({'label': y_test, 'text_a': X_test})\n\n# Save to new TSV files\ntrain_df.to_csv('tsv_file_split/dataset_tfc_packet_l_train.tsv', sep='\\t', index=False)\ntest_df.to_csv('tsv_file_split/dataset_tfc_packet_l_test.tsv', sep='\\t', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T16:24:59.902792Z","iopub.execute_input":"2025-07-07T16:24:59.903218Z","iopub.status.idle":"2025-07-07T16:25:01.530094Z","shell.execute_reply.started":"2025-07-07T16:24:59.903191Z","shell.execute_reply":"2025-07-07T16:25:01.529374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the TSV file\nimport pandas as pd\ntrain = pd.read_csv('/kaggle/working/tsv_file/dataset_tor_packet.tsv', sep='\\t')\ntrai = pd.read_csv('/kaggle/working/tsv_file/dataset_tor.tsv', sep='\\t')\ntrain[\"label\"].value_counts()\nlen(train['text_a'][0]),len(trai['text_a'][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T01:59:10.993729Z","iopub.execute_input":"2025-07-02T01:59:10.994033Z","iopub.status.idle":"2025-07-02T01:59:11.731054Z","shell.execute_reply.started":"2025-07-02T01:59:10.994009Z","shell.execute_reply":"2025-07-02T01:59:11.729984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# lstm models","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport pandas as pd\nimport os\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:43:57.750498Z","iopub.execute_input":"2025-09-28T07:43:57.750725Z","iopub.status.idle":"2025-09-28T07:44:26.324399Z","shell.execute_reply.started":"2025-09-28T07:43:57.750709Z","shell.execute_reply":"2025-09-28T07:44:26.323833Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#creer une liste de word for the embegings\npath1=\"/kaggle/working/tsv_file_split/dataset_ciciot_packet_train.tsv\"\npath2=\"/kaggle/working/tsv_file_split/dataset_tfc_packet_l_test.tsv\"\npath3=\"/kaggle/working/tsv_file_split/dataset_tfc_packet_l_train.tsv\"\npath4=\"/kaggle/working/tsv_file_split/dataset_tor_packet_test.tsv\"\npath5=\"/kaggle/working/tsv_file_split/dataset_tor_packet_train.tsv\"\npath6=\"/kaggle/working/tsv_file_split/dataset_ustf-tfc_packet_test.tsv\"\npath7=\"/kaggle/working/tsv_file_split/dataset_ustf-tfc_packet_train.tsv\"\n\npaths=[path1,path2,path3,path4,path5,path6,path7]\ndf=pd.read_csv(path1)\nlt=[]\nfor path in paths:\n    df=pd.read_csv(path,sep=\"\\t\")\n    lt.append(df)\ndf=pd.concat(lt)\nsentences=df.text_a.tolist()\nlist_word=[sentence.split() for sentence in sentences]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:44:56.197594Z","iopub.execute_input":"2025-09-28T07:44:56.197825Z","iopub.status.idle":"2025-09-28T07:45:00.840733Z","shell.execute_reply.started":"2025-09-28T07:44:56.197809Z","shell.execute_reply":"2025-09-28T07:45:00.840127Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#lets train our Word2vec Model\nmodel_emb=Word2Vec(list_word,vector_size=64,window=5,workers=4)\n#essayons d'embed o word\nmodel_emb.wv[\"4e98\"]\n#model_emb.save(\"word2vec_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:45:00.84177Z","iopub.execute_input":"2025-09-28T07:45:00.842063Z","iopub.status.idle":"2025-09-28T07:46:19.136883Z","shell.execute_reply.started":"2025-09-28T07:45:00.842041Z","shell.execute_reply":"2025-09-28T07:46:19.136104Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([ 0.5278087 ,  0.02324824, -0.30430895,  0.55657595, -1.2761108 ,\n        1.3570802 ,  0.22196038, -0.05355455,  0.10258319,  0.28360704,\n       -0.3542523 ,  0.10284703, -0.41621166,  0.009511  ,  0.78432107,\n       -0.38037065,  0.26946986,  0.3376374 ,  0.28275222, -0.46227115,\n       -1.2999579 , -0.03900266, -0.06075571,  0.97147536,  0.7425491 ,\n        0.6637179 ,  0.7277807 , -0.2122171 ,  1.1894776 ,  0.26970965,\n       -0.46994507, -0.37154552,  0.32101426, -0.08506291, -0.95035744,\n       -0.35429162,  0.24093682, -0.18618672,  0.05498379, -0.18210594,\n        0.00682116, -0.7330861 ,  0.2582237 , -0.20799921,  0.5713041 ,\n        0.02655398, -0.8263295 ,  0.49343908, -0.82526684, -0.6561987 ,\n        1.088343  , -0.30323336, -0.12514731,  0.44312707,  0.10751735,\n       -0.03526834,  0.39908347,  0.42476025,  0.12340546,  0.5504348 ,\n        0.501027  , -0.16386531, -0.6997198 ,  0.06461726], dtype=float32)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model_emb=Word2Vec.load(\"/kaggle/working/word2vec_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:46:19.13822Z","iopub.execute_input":"2025-09-28T07:46:19.138877Z","iopub.status.idle":"2025-09-28T07:46:19.64364Z","shell.execute_reply.started":"2025-09-28T07:46:19.138858Z","shell.execute_reply":"2025-09-28T07:46:19.642876Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#now je dois creer un dataframe ou store les sentences et words sous forme de vecteur\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n#construction du model lstm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.metrics import Precision, Recall\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Embedding, LSTM, Dense, Dropout, \n    MultiHeadAttention, LayerNormalization, \n    GlobalAveragePooling1D, Bidirectional\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:46:19.644331Z","iopub.execute_input":"2025-09-28T07:46:19.644538Z","iopub.status.idle":"2025-09-28T07:46:34.149156Z","shell.execute_reply.started":"2025-09-28T07:46:19.644523Z","shell.execute_reply":"2025-09-28T07:46:34.148593Z"}},"outputs":[{"name":"stderr","text":"2025-09-28 07:46:21.533385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759045581.766541      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759045581.841887      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#take the dataset in a pandas dataframe \ndf_iscx_vpn=pd.read_csv(\"/kaggle/working/tsv_file/dataset_iscx_vpn_packet.tsv\",sep=\"\\t\")\n#df_ciciot=pd.read_csv(\"/kaggle/working/tsv_file/dataset_ciciot_packet.tsv\",sep=\"\\t\")\ndf_ustf_tfc=pd.read_csv(\"/kaggle/working/tsv_file/dataset_ustf-tfc_packet.tsv\",sep=\"\\t\")\n#df_tor=pd.read_csv(\"/kaggle/working/tsv_file/dataset_tor_packet.tsv\",sep=\"\\t\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:38:22.944486Z","iopub.execute_input":"2025-09-27T10:38:22.94515Z","iopub.status.idle":"2025-09-27T10:38:23.506719Z","shell.execute_reply.started":"2025-09-27T10:38:22.945127Z","shell.execute_reply":"2025-09-27T10:38:23.5061Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df_ustf_tfc[\"label\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:38:25.415853Z","iopub.execute_input":"2025-09-27T10:38:25.416334Z","iopub.status.idle":"2025-09-27T10:38:25.422617Z","shell.execute_reply.started":"2025-09-27T10:38:25.416312Z","shell.execute_reply":"2025-09-27T10:38:25.421917Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"label\n0     5000\n1     5000\n2     5000\n3     5000\n4     5000\n5     5000\n7     5000\n8     5000\n9     4988\n10    1389\n6       62\nName: count, dtype: int64"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"def training_prep(df:pd.DataFrame):\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(df_iscx_vpn.text_a)\n    X_train=df.text_a\n    X_test=df.text_a\n    y_train=df.label\n    y_test=df.label\n    \n    #faire une liste d'index des mots par le plus frequent au moins frequent\n    X_train_seq=tokenizer.texts_to_sequences(X_train)\n    X_test_seq=tokenizer.texts_to_sequences(X_test)\n    \n    # Padding des sÃ©quences pour avoir une longueur fixe\n    max_length = max(len(seq) for seq in X_train_seq)\n    X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n    X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n    embedding_matrice=np.zeros((len(tokenizer.word_index)+1,model_emb.vector_size))\n    for word,i in tokenizer.word_index.items():\n        if word in model_emb.wv:\n            embedding_matrice[i]=model_emb.wv[word]\n            \n    nbre_classes=df.label.value_counts().count()\n    #transformer mes label avec one_hot_encoding\n    y_train_onehot=utils.to_categorical(y_train,num_classes=nbre_classes)\n    y_test_onehot=utils.to_categorical(y_test,num_classes=nbre_classes)\n\n    result= {\n    \"nbre_classes\": nbre_classes,\n    \"tokenizer\": tokenizer,\n    \"embedding_matrice\": embedding_matrice,\n    \"max_length\": max_length,\n    \"X_train_pad\": X_train_pad,\n    \"X_test_pad\": X_test_pad,\n    \"y_train_onehot\": y_train_onehot,\n    \"y_test_onehot\":y_test_onehot\n    }\n\n    \n    return result\n    \ndef evaluation(model, result,saving_path):\n    # Get predictions\n    pred_prob = model.predict(result[\"X_test_pad\"],batch_size=32)\n    y_pred = np.argmax(pred_prob, axis=1)\n    y_true = np.argmax(result[\"y_test_onehot\"], axis=1)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='weighted'\n    )\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Create results\n    results = {\n        \"acc\": float(accuracy),\n        \"weighted_pre\": float(precision),\n        \"weighted_rec\": float(recall),\n        \"weighted_f1\": float(f1),\n        \"cm\": cm.tolist()\n    }\n    \n    with open(f\"lsmt_results/{saving_path}.json\",\"w\") as file:\n        json.dump(results,file, indent=2)\n    # Print and return JSON\n    print(json.dumps(results, indent=2))\n    return results\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:46:34.150694Z","iopub.execute_input":"2025-09-28T07:46:34.15115Z","iopub.status.idle":"2025-09-28T07:46:34.159645Z","shell.execute_reply.started":"2025-09-28T07:46:34.151133Z","shell.execute_reply":"2025-09-28T07:46:34.158932Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model_lstm=Sequential()\nmodel_lstm.add(Embedding(input_dim=result[\"embedding_matrice\"].shape[0],\n                          output_dim=result[\"embedding_matrice\"].shape[1],\n                          weights=[result[\"embedding_matrice\"]],\n                          input_length=result[\"max_length\"] ,\n                          trainable=False\n                         ))\nmodel_lstm.add(LSTM(128,return_sequences=True))\nmodel_lstm.add(Dropout(0.2))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(Dropout(0.2))\nmodel_lstm.add(LSTM(32))\nmodel_lstm.add(Dense(result[\"nbre_classes\"],activation=\"softmax\"))\n\nmodel_lstm.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n\nmodel_lstm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T18:30:06.379551Z","iopub.execute_input":"2025-08-03T18:30:06.379974Z","iopub.status.idle":"2025-08-03T18:30:06.488423Z","shell.execute_reply.started":"2025-08-03T18:30:06.379946Z","shell.execute_reply":"2025-08-03T18:30:06.487436Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              â”‚ ?                           â”‚       \u001b[38;5;34m4,194,368\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                       â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_13 (\u001b[38;5;33mLSTM\u001b[0m)                       â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)                       â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              â”‚ ?                           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,194,368\u001b[0m (16.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> (16.00 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,194,368\u001b[0m (16.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> (16.00 MB)\n</pre>\n"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"model_lstm.fit(X_train_pad, y_train_onehot, epochs=20, batch_size=64, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T16:15:56.446555Z","iopub.execute_input":"2025-08-03T16:15:56.446923Z","iopub.status.idle":"2025-08-03T16:56:04.501085Z","shell.execute_reply.started":"2025-08-03T16:15:56.446896Z","shell.execute_reply":"2025-08-03T16:56:04.499953Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 349ms/step - accuracy: 0.4459 - loss: 1.2880 - val_accuracy: 0.0067 - val_loss: 6.2681\nEpoch 2/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 342ms/step - accuracy: 0.5200 - loss: 1.0923 - val_accuracy: 0.0484 - val_loss: 6.6255\nEpoch 3/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 335ms/step - accuracy: 0.6244 - loss: 0.8602 - val_accuracy: 0.0510 - val_loss: 8.0539\nEpoch 4/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 336ms/step - accuracy: 0.6436 - loss: 0.8097 - val_accuracy: 0.0758 - val_loss: 8.4561\nEpoch 5/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 349ms/step - accuracy: 0.6817 - loss: 0.7315 - val_accuracy: 0.0760 - val_loss: 8.3994\nEpoch 6/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 335ms/step - accuracy: 0.6799 - loss: 0.7291 - val_accuracy: 0.0468 - val_loss: 8.6970\nEpoch 7/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 338ms/step - accuracy: 0.6832 - loss: 0.7189 - val_accuracy: 0.0746 - val_loss: 8.7641\nEpoch 8/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 333ms/step - accuracy: 0.6889 - loss: 0.7113 - val_accuracy: 0.0755 - val_loss: 8.4661\nEpoch 9/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 333ms/step - accuracy: 0.6928 - loss: 0.7006 - val_accuracy: 0.0749 - val_loss: 9.1378\nEpoch 10/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 335ms/step - accuracy: 0.6923 - loss: 0.6997 - val_accuracy: 0.0749 - val_loss: 9.5232\nEpoch 11/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 334ms/step - accuracy: 0.6978 - loss: 0.6840 - val_accuracy: 0.0776 - val_loss: 9.6501\nEpoch 12/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 342ms/step - accuracy: 0.6964 - loss: 0.6835 - val_accuracy: 0.0436 - val_loss: 10.1519\nEpoch 13/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 343ms/step - accuracy: 0.6984 - loss: 0.6892 - val_accuracy: 0.0741 - val_loss: 10.2669\nEpoch 14/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 335ms/step - accuracy: 0.6983 - loss: 0.6801 - val_accuracy: 0.0773 - val_loss: 10.4719\nEpoch 15/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 353ms/step - accuracy: 0.7055 - loss: 0.6682 - val_accuracy: 0.0751 - val_loss: 10.7203\nEpoch 16/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 345ms/step - accuracy: 0.7055 - loss: 0.6583 - val_accuracy: 0.0739 - val_loss: 10.9980\nEpoch 17/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 341ms/step - accuracy: 0.7166 - loss: 0.6502 - val_accuracy: 0.0911 - val_loss: 11.4066\nEpoch 18/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 337ms/step - accuracy: 0.7181 - loss: 0.6402 - val_accuracy: 0.0751 - val_loss: 11.6094\nEpoch 19/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 336ms/step - accuracy: 0.7290 - loss: 0.6353 - val_accuracy: 0.0737 - val_loss: 11.8152\nEpoch 20/20\n\u001b[1m353/353\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 341ms/step - accuracy: 0.7328 - loss: 0.6177 - val_accuracy: 0.0840 - val_loss: 11.9568\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7977a1456990>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"#loading the model\n# Dummy input (batch size 1, sequence length = max_length)\ndummy_input = np.zeros((1, result[\"max_length\"]), dtype=np.int32)\nmodel_lstm(dummy_input)  # This builds the model\nmodel_lstm.load_weights(\"lstm_models/lstm_base_iscx_vpn.weights.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model_lstm.save_weights(\"lstm_models/lstm_base_iscx_vpn.weights.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:26:43.881875Z","iopub.execute_input":"2025-08-03T17:26:43.882211Z","iopub.status.idle":"2025-08-03T17:26:44.022457Z","shell.execute_reply.started":"2025-08-03T17:26:43.882187Z","shell.execute_reply":"2025-08-03T17:26:44.021377Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## lstm+attention","metadata":{}},{"cell_type":"code","source":"def create_lstm_attention_model(embedding_matrice, max_length, nbre_classes):\n    \"\"\"\n    Creates LSTM + Multi-Head Attention model for network classification\n    \n    Args:\n        embedding_matrice: Pre-trained embedding matrix\n        max_length: Maximum sequence length\n        nbre_classes: Number of output classes\n    \n    Returns:\n        Compiled Keras model\n    \"\"\"\n    inputs = Input(shape=(max_length,))\n    \n    # Embedding layer (frozen pre-trained embeddings)\n    embedding = Embedding(\n        input_dim=embedding_matrice.shape[0],\n        output_dim=embedding_matrice.shape[1],\n        weights=[embedding_matrice],\n        input_length=max_length,\n        trainable=False\n    )(inputs)\n    \n    # LSTM layers for sequential pattern extraction\n    lstm1 = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n    lstm1 = Dropout(0.3)(lstm1)\n    \n    lstm2 = Bidirectional(LSTM(64, return_sequences=True))(lstm1)\n    lstm2 = Dropout(0.3)(lstm2)\n    \n    # Multi-head attention on LSTM outputs\n    attention = MultiHeadAttention(\n        num_heads=8,           # 8 attention heads\n        key_dim=64,           # Dimension of each head\n        \n        dropout=0.1\n    )(lstm2, lstm2)           # Self-attention on LSTM outputs\n    \n    # Residual connection + Layer normalization\n    attention_output = LayerNormalization()(attention + lstm2)\n    \n    # Global pooling to reduce sequence dimension\n    pooled = GlobalAveragePooling1D()(attention_output)\n    \n    # Classification head\n    dense1 = Dense(128, activation='relu')(pooled)\n    dense1 = Dropout(0.5)(dense1)\n    \n    outputs = Dense(nbre_classes, activation='softmax')(dense1)\n    \n    # Create model\n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Compile with same settings as your original + additional metrics\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy', Precision(), Recall()]\n    )\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:46:34.160447Z","iopub.execute_input":"2025-09-28T07:46:34.160731Z","iopub.status.idle":"2025-09-28T07:46:34.194029Z","shell.execute_reply.started":"2025-09-28T07:46:34.160708Z","shell.execute_reply":"2025-09-28T07:46:34.193461Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Optional: Training callbacks for better performance\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',\n        patience=50,\n        restore_best_weights=True\n    ),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-15\n    )\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:46:34.194735Z","iopub.execute_input":"2025-09-28T07:46:34.194907Z","iopub.status.idle":"2025-09-28T07:46:34.21148Z","shell.execute_reply.started":"2025-09-28T07:46:34.194893Z","shell.execute_reply":"2025-09-28T07:46:34.210881Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#take the dataset in a pandas dataframe \ndf_iscx_vpn=pd.read_csv(\"/kaggle/working/tsv_file/dataset_iscxvpn_packet_l.tsv\",sep=\"\\t\")\ndf_ciciot=pd.read_csv(\"/kaggle/working/tsv_file/dataset_ciciot_packet.tsv\",sep=\"\\t\")\ndf_ustf_tfc=pd.read_csv(\"/kaggle/working/tsv_file/dataset_ustf-tfc_packet_l.tsv\",sep=\"\\t\")\ndf_tor=pd.read_csv(\"/kaggle/working/tsv_file/dataset_tor_packet.tsv\",sep=\"\\t\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:12:14.518267Z","iopub.execute_input":"2025-09-28T08:12:14.518566Z","iopub.status.idle":"2025-09-28T08:12:15.594043Z","shell.execute_reply.started":"2025-09-28T08:12:14.518546Z","shell.execute_reply":"2025-09-28T08:12:15.593462Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"df_iscx_vpn[\"label\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:12:15.595156Z","iopub.execute_input":"2025-09-28T08:12:15.595401Z","iopub.status.idle":"2025-09-28T08:12:15.601761Z","shell.execute_reply.started":"2025-09-28T08:12:15.59538Z","shell.execute_reply":"2025-09-28T08:12:15.601147Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"label\n2    5000\n4    5000\n3    5000\n5    5000\n0    5000\n1    5000\n6    5000\nName: count, dtype: int64"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Create your model\ndef model_initiation(df):\n    result=training_prep(df)\n    model_lstm_attention = create_lstm_attention_model(result[\"embedding_matrice\"], result[\"max_length\"], result[\"nbre_classes\"])\n    # Display architecture\n    \n    return model_lstm_attention,result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:46:35.316361Z","iopub.execute_input":"2025-09-28T07:46:35.317265Z","iopub.status.idle":"2025-09-28T07:46:35.32066Z","shell.execute_reply.started":"2025-09-28T07:46:35.317244Z","shell.execute_reply":"2025-09-28T07:46:35.320113Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Train the model iscx_vpn \nmodel_lstm_attention_iscx,result=model_initiation(df_iscx_vpn)\nmodel_lstm_attention_iscx.summary()\nhistory = model_lstm_attention_iscx.fit(\n     result[\"X_train_pad\"], result[\"y_train_onehot\"],\n     validation_split=0.2,\n     epochs=50,\n     batch_size=32,\n     callbacks=callbacks\n )\nmodel_lstm_attention_iscx.save_weights(\"lstm_models/lstm_plus_attention_iscx_vpn_l.weights.h5\")\n\nevaluation(model_lstm_attention_iscx,result,\"model_lstm_attention_iscx_result_l\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:15:36.561781Z","iopub.execute_input":"2025-09-28T08:15:36.562098Z","iopub.status.idle":"2025-09-28T08:30:27.975046Z","shell.execute_reply.started":"2025-09-28T08:15:36.562077Z","shell.execute_reply":"2025-09-28T08:30:27.974461Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_3             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚      \u001b[38;5;34m4,194,368\u001b[0m â”‚ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_6           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        â”‚        \u001b[38;5;34m197,632\u001b[0m â”‚ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_7           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m164,352\u001b[0m â”‚ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention_3    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m263,808\u001b[0m â”‚ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      â”‚\nâ”‚ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚                        â”‚                â”‚ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_3 (\u001b[38;5;33mAdd\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚              \u001b[38;5;34m0\u001b[0m â”‚ multi_head_attention_â€¦ â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization_3     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚            \u001b[38;5;34m256\u001b[0m â”‚ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\nâ”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1dâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_3â€¦ â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚         \u001b[38;5;34m16,512\u001b[0m â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              â”‚            \u001b[38;5;34m903\u001b[0m â”‚ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_3             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> â”‚ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_6           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> â”‚ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_7           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention_3    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> â”‚ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚                        â”‚                â”‚ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_head_attention_â€¦ â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization_3     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1dâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_3â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> â”‚ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,837,831\u001b[0m (18.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,837,831</span> (18.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m643,463\u001b[0m (2.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">643,463</span> (2.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,194,368\u001b[0m (16.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> (16.00 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - accuracy: 0.8567 - loss: 0.4187 - precision_3: 0.9041 - recall_3: 0.8167 - val_accuracy: 0.2737 - val_loss: 11.9618 - val_precision_3: 0.2764 - val_recall_3: 0.2733 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9585 - loss: 0.1173 - precision_3: 0.9684 - recall_3: 0.9497 - val_accuracy: 0.2756 - val_loss: 11.9166 - val_precision_3: 0.2765 - val_recall_3: 0.2750 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9677 - loss: 0.0837 - precision_3: 0.9771 - recall_3: 0.9604 - val_accuracy: 0.2767 - val_loss: 16.1389 - val_precision_3: 0.2829 - val_recall_3: 0.2760 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9717 - loss: 0.0688 - precision_3: 0.9816 - recall_3: 0.9646 - val_accuracy: 0.2760 - val_loss: 11.0302 - val_precision_3: 0.2758 - val_recall_3: 0.2716 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9723 - loss: 0.0673 - precision_3: 0.9848 - recall_3: 0.9636 - val_accuracy: 0.2776 - val_loss: 14.2461 - val_precision_3: 0.2781 - val_recall_3: 0.2736 - learning_rate: 0.0010\nEpoch 6/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9772 - loss: 0.0545 - precision_3: 0.9851 - recall_3: 0.9695 - val_accuracy: 0.2791 - val_loss: 14.1502 - val_precision_3: 0.2817 - val_recall_3: 0.2780 - learning_rate: 0.0010\nEpoch 7/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9796 - loss: 0.0472 - precision_3: 0.9857 - recall_3: 0.9738 - val_accuracy: 0.2790 - val_loss: 18.6648 - val_precision_3: 0.2891 - val_recall_3: 0.2781 - learning_rate: 0.0010\nEpoch 8/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9840 - loss: 0.0344 - precision_3: 0.9887 - recall_3: 0.9800 - val_accuracy: 0.2789 - val_loss: 13.6745 - val_precision_3: 0.2828 - val_recall_3: 0.2786 - learning_rate: 5.0000e-04\nEpoch 9/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9848 - loss: 0.0308 - precision_3: 0.9875 - recall_3: 0.9828 - val_accuracy: 0.2797 - val_loss: 20.8683 - val_precision_3: 0.2820 - val_recall_3: 0.2793 - learning_rate: 5.0000e-04\nEpoch 10/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9867 - loss: 0.0274 - precision_3: 0.9882 - recall_3: 0.9846 - val_accuracy: 0.2793 - val_loss: 19.4930 - val_precision_3: 0.2795 - val_recall_3: 0.2790 - learning_rate: 5.0000e-04\nEpoch 11/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9884 - loss: 0.0241 - precision_3: 0.9899 - recall_3: 0.9867 - val_accuracy: 0.2796 - val_loss: 23.6074 - val_precision_3: 0.2794 - val_recall_3: 0.2793 - learning_rate: 2.5000e-04\nEpoch 12/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9867 - loss: 0.0247 - precision_3: 0.9887 - recall_3: 0.9854 - val_accuracy: 0.2800 - val_loss: 24.2087 - val_precision_3: 0.2801 - val_recall_3: 0.2799 - learning_rate: 2.5000e-04\nEpoch 13/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9894 - loss: 0.0191 - precision_3: 0.9900 - recall_3: 0.9886 - val_accuracy: 0.2796 - val_loss: 26.5809 - val_precision_3: 0.2796 - val_recall_3: 0.2796 - learning_rate: 2.5000e-04\nEpoch 14/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9884 - loss: 0.0200 - precision_3: 0.9893 - recall_3: 0.9873 - val_accuracy: 0.2799 - val_loss: 27.4257 - val_precision_3: 0.2798 - val_recall_3: 0.2797 - learning_rate: 1.2500e-04\nEpoch 15/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9895 - loss: 0.0191 - precision_3: 0.9903 - recall_3: 0.9889 - val_accuracy: 0.2806 - val_loss: 27.7687 - val_precision_3: 0.2804 - val_recall_3: 0.2803 - learning_rate: 1.2500e-04\nEpoch 16/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9903 - loss: 0.0169 - precision_3: 0.9912 - recall_3: 0.9893 - val_accuracy: 0.2801 - val_loss: 28.9667 - val_precision_3: 0.2801 - val_recall_3: 0.2800 - learning_rate: 1.2500e-04\nEpoch 17/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9892 - loss: 0.0173 - precision_3: 0.9894 - recall_3: 0.9889 - val_accuracy: 0.2801 - val_loss: 29.7641 - val_precision_3: 0.2800 - val_recall_3: 0.2799 - learning_rate: 6.2500e-05\nEpoch 18/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9911 - loss: 0.0159 - precision_3: 0.9915 - recall_3: 0.9906 - val_accuracy: 0.2807 - val_loss: 30.0080 - val_precision_3: 0.2807 - val_recall_3: 0.2807 - learning_rate: 6.2500e-05\nEpoch 19/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9922 - loss: 0.0156 - precision_3: 0.9925 - recall_3: 0.9920 - val_accuracy: 0.2807 - val_loss: 30.8864 - val_precision_3: 0.2807 - val_recall_3: 0.2807 - learning_rate: 6.2500e-05\nEpoch 20/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9917 - loss: 0.0151 - precision_3: 0.9921 - recall_3: 0.9912 - val_accuracy: 0.2809 - val_loss: 31.0042 - val_precision_3: 0.2809 - val_recall_3: 0.2809 - learning_rate: 3.1250e-05\nEpoch 21/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9926 - loss: 0.0150 - precision_3: 0.9929 - recall_3: 0.9920 - val_accuracy: 0.2810 - val_loss: 31.3570 - val_precision_3: 0.2809 - val_recall_3: 0.2809 - learning_rate: 3.1250e-05\nEpoch 22/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9931 - loss: 0.0136 - precision_3: 0.9934 - recall_3: 0.9929 - val_accuracy: 0.2807 - val_loss: 31.7426 - val_precision_3: 0.2807 - val_recall_3: 0.2807 - learning_rate: 3.1250e-05\nEpoch 23/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9923 - loss: 0.0155 - precision_3: 0.9928 - recall_3: 0.9921 - val_accuracy: 0.2810 - val_loss: 31.7086 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 1.5625e-05\nEpoch 24/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9917 - loss: 0.0149 - precision_3: 0.9918 - recall_3: 0.9915 - val_accuracy: 0.2810 - val_loss: 31.6209 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 1.5625e-05\nEpoch 25/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9932 - loss: 0.0129 - precision_3: 0.9935 - recall_3: 0.9928 - val_accuracy: 0.2810 - val_loss: 31.7658 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 1.5625e-05\nEpoch 26/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9927 - loss: 0.0142 - precision_3: 0.9928 - recall_3: 0.9926 - val_accuracy: 0.2810 - val_loss: 31.7597 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 7.8125e-06\nEpoch 27/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9918 - loss: 0.0146 - precision_3: 0.9920 - recall_3: 0.9915 - val_accuracy: 0.2811 - val_loss: 31.8033 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 7.8125e-06\nEpoch 28/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9930 - loss: 0.0133 - precision_3: 0.9932 - recall_3: 0.9927 - val_accuracy: 0.2810 - val_loss: 31.9034 - val_precision_3: 0.2809 - val_recall_3: 0.2809 - learning_rate: 7.8125e-06\nEpoch 29/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9929 - loss: 0.0132 - precision_3: 0.9930 - recall_3: 0.9929 - val_accuracy: 0.2810 - val_loss: 31.9586 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 3.9063e-06\nEpoch 30/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9908 - loss: 0.0153 - precision_3: 0.9912 - recall_3: 0.9906 - val_accuracy: 0.2809 - val_loss: 32.0127 - val_precision_3: 0.2809 - val_recall_3: 0.2809 - learning_rate: 3.9063e-06\nEpoch 31/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9937 - loss: 0.0132 - precision_3: 0.9939 - recall_3: 0.9935 - val_accuracy: 0.2809 - val_loss: 32.0253 - val_precision_3: 0.2809 - val_recall_3: 0.2809 - learning_rate: 3.9063e-06\nEpoch 32/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9936 - loss: 0.0130 - precision_3: 0.9939 - recall_3: 0.9935 - val_accuracy: 0.2811 - val_loss: 32.0496 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 1.9531e-06\nEpoch 33/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9924 - loss: 0.0149 - precision_3: 0.9926 - recall_3: 0.9922 - val_accuracy: 0.2810 - val_loss: 32.0522 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 1.9531e-06\nEpoch 34/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9936 - loss: 0.0133 - precision_3: 0.9938 - recall_3: 0.9933 - val_accuracy: 0.2810 - val_loss: 32.0770 - val_precision_3: 0.2809 - val_recall_3: 0.2809 - learning_rate: 1.9531e-06\nEpoch 35/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9921 - loss: 0.0147 - precision_3: 0.9922 - recall_3: 0.9919 - val_accuracy: 0.2810 - val_loss: 32.0813 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 9.7656e-07\nEpoch 36/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9927 - loss: 0.0136 - precision_3: 0.9928 - recall_3: 0.9924 - val_accuracy: 0.2810 - val_loss: 32.0882 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 9.7656e-07\nEpoch 37/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9926 - loss: 0.0140 - precision_3: 0.9926 - recall_3: 0.9923 - val_accuracy: 0.2810 - val_loss: 32.0882 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 9.7656e-07\nEpoch 38/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9929 - loss: 0.0141 - precision_3: 0.9930 - recall_3: 0.9927 - val_accuracy: 0.2810 - val_loss: 32.0852 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 4.8828e-07\nEpoch 39/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9933 - loss: 0.0126 - precision_3: 0.9934 - recall_3: 0.9931 - val_accuracy: 0.2810 - val_loss: 32.0856 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 4.8828e-07\nEpoch 40/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9915 - loss: 0.0143 - precision_3: 0.9917 - recall_3: 0.9914 - val_accuracy: 0.2810 - val_loss: 32.0894 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 4.8828e-07\nEpoch 41/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9931 - loss: 0.0135 - precision_3: 0.9934 - recall_3: 0.9928 - val_accuracy: 0.2810 - val_loss: 32.0910 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 2.4414e-07\nEpoch 42/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9928 - loss: 0.0135 - precision_3: 0.9929 - recall_3: 0.9925 - val_accuracy: 0.2810 - val_loss: 32.0903 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 2.4414e-07\nEpoch 43/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9926 - loss: 0.0145 - precision_3: 0.9928 - recall_3: 0.9924 - val_accuracy: 0.2810 - val_loss: 32.0912 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 2.4414e-07\nEpoch 44/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9933 - loss: 0.0120 - precision_3: 0.9933 - recall_3: 0.9932 - val_accuracy: 0.2810 - val_loss: 32.0936 - val_precision_3: 0.2811 - val_recall_3: 0.2810 - learning_rate: 1.2207e-07\nEpoch 45/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9915 - loss: 0.0153 - precision_3: 0.9916 - recall_3: 0.9915 - val_accuracy: 0.2810 - val_loss: 32.0938 - val_precision_3: 0.2811 - val_recall_3: 0.2810 - learning_rate: 1.2207e-07\nEpoch 46/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9929 - loss: 0.0130 - precision_3: 0.9929 - recall_3: 0.9927 - val_accuracy: 0.2810 - val_loss: 32.0939 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 1.2207e-07\nEpoch 47/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9930 - loss: 0.0143 - precision_3: 0.9932 - recall_3: 0.9927 - val_accuracy: 0.2810 - val_loss: 32.0940 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 6.1035e-08\nEpoch 48/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9930 - loss: 0.0128 - precision_3: 0.9933 - recall_3: 0.9929 - val_accuracy: 0.2810 - val_loss: 32.0948 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 6.1035e-08\nEpoch 49/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9933 - loss: 0.0133 - precision_3: 0.9934 - recall_3: 0.9932 - val_accuracy: 0.2810 - val_loss: 32.0944 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 6.1035e-08\nEpoch 50/50\n\u001b[1m875/875\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9927 - loss: 0.0132 - precision_3: 0.9929 - recall_3: 0.9926 - val_accuracy: 0.2810 - val_loss: 32.0947 - val_precision_3: 0.2810 - val_recall_3: 0.2810 - learning_rate: 3.0518e-08\n\u001b[1m1094/1094\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step\n{\n  \"acc\": 0.8508,\n  \"weighted_pre\": 0.7723857865992797,\n  \"weighted_rec\": 0.8508,\n  \"weighted_f1\": 0.8005426895983093,\n  \"cm\": [\n    [\n      4941,\n      59,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      133,\n      4866,\n      0,\n      1,\n      0,\n      0,\n      0\n    ],\n    [\n      8,\n      2,\n      4981,\n      0,\n      0,\n      9,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0\n    ],\n    [\n      3,\n      0,\n      7,\n      0,\n      0,\n      4990,\n      0\n    ],\n    [\n      0,\n      0,\n      339,\n      0,\n      4639,\n      22,\n      0\n    ]\n  ]\n}\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'acc': 0.8508,\n 'weighted_pre': 0.7723857865992797,\n 'weighted_rec': 0.8508,\n 'weighted_f1': 0.8005426895983093,\n 'cm': [[4941, 59, 0, 0, 0, 0, 0],\n  [133, 4866, 0, 1, 0, 0, 0],\n  [8, 2, 4981, 0, 0, 9, 0],\n  [0, 0, 0, 5000, 0, 0, 0],\n  [0, 0, 0, 0, 5000, 0, 0],\n  [3, 0, 7, 0, 0, 4990, 0],\n  [0, 0, 339, 0, 4639, 22, 0]]}"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"#import the model\nmodel_lstm_attention_isxc,result=model_initiation(df_iscx_vpn)\nmodel_lstm_attention_isxc.load_weights(\"lstm_models/lstm_plus_attention_iscx_vpn_l.weights.h5\")\nevaluation(model_lstm_attention_isxc,result,\"model_lstm_attention_iscx_result_l_temp\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:47:37.53492Z","iopub.execute_input":"2025-09-28T08:47:37.535479Z","iopub.status.idle":"2025-09-28T08:47:49.732246Z","shell.execute_reply.started":"2025-09-28T08:47:37.535456Z","shell.execute_reply":"2025-09-28T08:47:49.731617Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"\u001b[1m1094/1094\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step\n{\n  \"acc\": 0.8508,\n  \"weighted_pre\": 0.7723857865992797,\n  \"weighted_rec\": 0.8508,\n  \"weighted_f1\": 0.8005426895983093,\n  \"cm\": [\n    [\n      4941,\n      59,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      133,\n      4866,\n      0,\n      1,\n      0,\n      0,\n      0\n    ],\n    [\n      8,\n      2,\n      4981,\n      0,\n      0,\n      9,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0\n    ],\n    [\n      3,\n      0,\n      7,\n      0,\n      0,\n      4990,\n      0\n    ],\n    [\n      0,\n      0,\n      339,\n      0,\n      4639,\n      22,\n      0\n    ]\n  ]\n}\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'acc': 0.8508,\n 'weighted_pre': 0.7723857865992797,\n 'weighted_rec': 0.8508,\n 'weighted_f1': 0.8005426895983093,\n 'cm': [[4941, 59, 0, 0, 0, 0, 0],\n  [133, 4866, 0, 1, 0, 0, 0],\n  [8, 2, 4981, 0, 0, 9, 0],\n  [0, 0, 0, 5000, 0, 0, 0],\n  [0, 0, 0, 0, 5000, 0, 0],\n  [3, 0, 7, 0, 0, 4990, 0],\n  [0, 0, 339, 0, 4639, 22, 0]]}"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"df_iscx_vpn.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:47:08.410921Z","iopub.execute_input":"2025-09-28T08:47:08.41146Z","iopub.status.idle":"2025-09-28T08:47:08.41809Z","shell.execute_reply.started":"2025-09-28T08:47:08.411419Z","shell.execute_reply":"2025-09-28T08:47:08.417261Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"label\n2    5000\n4    5000\n3    5000\n5    5000\n0    5000\n1    5000\n6    5000\nName: count, dtype: int64"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# Train the model ustf_tfc\nmodel_lstm_attention_ustf,result=model_initiation(df_ustf_tfc)\nmodel_lstm_attention_ustf.summary()\nhistory = model_lstm_attention_ustf.fit(\n     result[\"X_train_pad\"], result[\"y_train_onehot\"],\n     validation_split=0.2,\n     epochs=50,\n     batch_size=32,\n     callbacks=callbacks\n )\nmodel_lstm_attention_ustf.save_weights(\"lstm_models/lstm_plus_attention_ustf_tfc.weights.h5\")\n\nevaluation(model_lstm_attention_ustf,result,\"model_lstm_attention_ustf_result\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:41:03.968632Z","iopub.execute_input":"2025-09-27T10:41:03.968902Z","iopub.status.idle":"2025-09-27T11:24:01.287068Z","shell.execute_reply.started":"2025-09-27T10:41:03.968883Z","shell.execute_reply":"2025-09-27T11:24:01.286081Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"I0000 00:00:1758969672.704338      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1758969672.705036      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚      \u001b[38;5;34m4,194,368\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        â”‚        \u001b[38;5;34m197,632\u001b[0m â”‚ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m256\u001b[0m)        â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m164,352\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m263,808\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       â”‚\nâ”‚ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚                        â”‚                â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (\u001b[38;5;33mAdd\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚              \u001b[38;5;34m0\u001b[0m â”‚ multi_head_attention[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚            \u001b[38;5;34m256\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\nâ”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1d  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚         \u001b[38;5;34m16,512\u001b[0m â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             â”‚          \u001b[38;5;34m2,580\u001b[0m â”‚ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> â”‚ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚                        â”‚                â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1d  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580</span> â”‚ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,839,508\u001b[0m (18.46 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,839,508</span> (18.46 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m645,140\u001b[0m (2.46 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">645,140</span> (2.46 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,194,368\u001b[0m (16.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> (16.00 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1758969684.068599     131 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 22ms/step - accuracy: 0.8629 - loss: 0.4242 - precision: 0.9343 - recall: 0.8148 - val_accuracy: 0.9608 - val_loss: 0.1054 - val_precision: 0.9636 - val_recall: 0.9583 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9614 - loss: 0.0995 - precision: 0.9665 - recall: 0.9568 - val_accuracy: 0.9701 - val_loss: 0.0726 - val_precision: 0.9717 - val_recall: 0.9677 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9718 - loss: 0.0733 - precision: 0.9743 - recall: 0.9692 - val_accuracy: 0.9724 - val_loss: 0.0728 - val_precision: 0.9757 - val_recall: 0.9675 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9759 - loss: 0.0620 - precision: 0.9785 - recall: 0.9730 - val_accuracy: 0.9812 - val_loss: 0.0495 - val_precision: 0.9825 - val_recall: 0.9804 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9806 - loss: 0.0506 - precision: 0.9824 - recall: 0.9791 - val_accuracy: 0.9813 - val_loss: 0.0463 - val_precision: 0.9821 - val_recall: 0.9807 - learning_rate: 0.0010\nEpoch 6/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9836 - loss: 0.0415 - precision: 0.9849 - recall: 0.9821 - val_accuracy: 0.9832 - val_loss: 0.0496 - val_precision: 0.9835 - val_recall: 0.9831 - learning_rate: 0.0010\nEpoch 7/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9857 - loss: 0.0387 - precision: 0.9870 - recall: 0.9847 - val_accuracy: 0.9849 - val_loss: 0.0396 - val_precision: 0.9856 - val_recall: 0.9845 - learning_rate: 0.0010\nEpoch 8/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9868 - loss: 0.0327 - precision: 0.9879 - recall: 0.9859 - val_accuracy: 0.9835 - val_loss: 0.0426 - val_precision: 0.9840 - val_recall: 0.9831 - learning_rate: 0.0010\nEpoch 9/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9889 - loss: 0.0284 - precision: 0.9897 - recall: 0.9878 - val_accuracy: 0.9855 - val_loss: 0.0446 - val_precision: 0.9867 - val_recall: 0.9847 - learning_rate: 0.0010\nEpoch 10/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9893 - loss: 0.0274 - precision: 0.9903 - recall: 0.9885 - val_accuracy: 0.9850 - val_loss: 0.0465 - val_precision: 0.9858 - val_recall: 0.9839 - learning_rate: 0.0010\nEpoch 11/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9927 - loss: 0.0183 - precision: 0.9932 - recall: 0.9920 - val_accuracy: 0.9866 - val_loss: 0.0482 - val_precision: 0.9869 - val_recall: 0.9864 - learning_rate: 5.0000e-04\nEpoch 12/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9950 - loss: 0.0134 - precision: 0.9952 - recall: 0.9947 - val_accuracy: 0.9873 - val_loss: 0.0480 - val_precision: 0.9878 - val_recall: 0.9870 - learning_rate: 5.0000e-04\nEpoch 13/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 21ms/step - accuracy: 0.9962 - loss: 0.0105 - precision: 0.9965 - recall: 0.9961 - val_accuracy: 0.9880 - val_loss: 0.0481 - val_precision: 0.9882 - val_recall: 0.9877 - learning_rate: 5.0000e-04\nEpoch 14/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9976 - loss: 0.0071 - precision: 0.9977 - recall: 0.9975 - val_accuracy: 0.9876 - val_loss: 0.0681 - val_precision: 0.9877 - val_recall: 0.9876 - learning_rate: 2.5000e-04\nEpoch 15/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9983 - loss: 0.0051 - precision: 0.9984 - recall: 0.9982 - val_accuracy: 0.9879 - val_loss: 0.0809 - val_precision: 0.9880 - val_recall: 0.9879 - learning_rate: 2.5000e-04\nEpoch 16/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0044 - precision: 0.9986 - recall: 0.9984 - val_accuracy: 0.9879 - val_loss: 0.0707 - val_precision: 0.9881 - val_recall: 0.9878 - learning_rate: 2.5000e-04\nEpoch 17/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9993 - loss: 0.0022 - precision: 0.9994 - recall: 0.9993 - val_accuracy: 0.9880 - val_loss: 0.0882 - val_precision: 0.9882 - val_recall: 0.9880 - learning_rate: 1.2500e-04\nEpoch 18/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9995 - loss: 0.0016 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9880 - val_loss: 0.0995 - val_precision: 0.9881 - val_recall: 0.9880 - learning_rate: 1.2500e-04\nEpoch 19/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 9.4338e-04 - precision: 0.9997 - recall: 0.9997 - val_accuracy: 0.9875 - val_loss: 0.1136 - val_precision: 0.9877 - val_recall: 0.9874 - learning_rate: 1.2500e-04\nEpoch 20/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9996 - loss: 0.0015 - precision: 0.9997 - recall: 0.9995 - val_accuracy: 0.9881 - val_loss: 0.1263 - val_precision: 0.9882 - val_recall: 0.9881 - learning_rate: 6.2500e-05\nEpoch 21/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9998 - loss: 7.4233e-04 - precision: 0.9998 - recall: 0.9998 - val_accuracy: 0.9880 - val_loss: 0.1237 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 6.2500e-05\nEpoch 22/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9998 - loss: 6.3084e-04 - precision: 0.9998 - recall: 0.9998 - val_accuracy: 0.9875 - val_loss: 0.1294 - val_precision: 0.9876 - val_recall: 0.9875 - learning_rate: 6.2500e-05\nEpoch 23/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9998 - loss: 5.3696e-04 - precision: 0.9998 - recall: 0.9998 - val_accuracy: 0.9882 - val_loss: 0.1342 - val_precision: 0.9882 - val_recall: 0.9882 - learning_rate: 3.1250e-05\nEpoch 24/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9998 - loss: 4.0815e-04 - precision: 0.9999 - recall: 0.9998 - val_accuracy: 0.9883 - val_loss: 0.1387 - val_precision: 0.9883 - val_recall: 0.9883 - learning_rate: 3.1250e-05\nEpoch 25/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 3.6548e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9883 - val_loss: 0.1448 - val_precision: 0.9883 - val_recall: 0.9883 - learning_rate: 3.1250e-05\nEpoch 26/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 5.6551e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1469 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 1.5625e-05\nEpoch 27/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 2.4239e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9878 - val_loss: 0.1528 - val_precision: 0.9878 - val_recall: 0.9878 - learning_rate: 1.5625e-05\nEpoch 28/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 1.5322e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9878 - val_loss: 0.1574 - val_precision: 0.9878 - val_recall: 0.9878 - learning_rate: 1.5625e-05\nEpoch 29/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.6529e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1588 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 7.8125e-06\nEpoch 30/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.2385e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9881 - val_loss: 0.1606 - val_precision: 0.9881 - val_recall: 0.9881 - learning_rate: 7.8125e-06\nEpoch 31/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 2.4250e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9881 - val_loss: 0.1612 - val_precision: 0.9881 - val_recall: 0.9881 - learning_rate: 7.8125e-06\nEpoch 32/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 2.7732e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1618 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 3.9063e-06\nEpoch 33/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 1.7327e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1625 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 3.9063e-06\nEpoch 34/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.3625e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1629 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 3.9063e-06\nEpoch 35/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.1283e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9879 - val_loss: 0.1635 - val_precision: 0.9879 - val_recall: 0.9879 - learning_rate: 1.9531e-06\nEpoch 36/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 8.0366e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1639 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 1.9531e-06\nEpoch 37/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 7.9741e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9881 - val_loss: 0.1646 - val_precision: 0.9881 - val_recall: 0.9881 - learning_rate: 1.9531e-06\nEpoch 38/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 6.7530e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9881 - val_loss: 0.1649 - val_precision: 0.9881 - val_recall: 0.9881 - learning_rate: 9.7656e-07\nEpoch 39/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 9.6646e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1650 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 9.7656e-07\nEpoch 40/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.0066e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1652 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 9.7656e-07\nEpoch 41/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 7.8221e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1652 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 4.8828e-07\nEpoch 42/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 1.3824e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1654 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 4.8828e-07\nEpoch 43/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 2.4860e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1654 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 4.8828e-07\nEpoch 44/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 1.6585e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1655 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 2.4414e-07\nEpoch 45/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.1832e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1655 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 2.4414e-07\nEpoch 46/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.5370e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1656 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 2.4414e-07\nEpoch 47/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 9.4636e-05 - precision: 1.0000 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1656 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 1.2207e-07\nEpoch 48/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 7.9102e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1656 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 1.2207e-07\nEpoch 49/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 2.5219e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9880 - val_loss: 0.1657 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 1.2207e-07\nEpoch 50/50\n\u001b[1m2428/2428\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 7.1040e-05 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9880 - val_loss: 0.1657 - val_precision: 0.9880 - val_recall: 0.9880 - learning_rate: 6.1035e-08\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3749890687.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_lstm_attention_ustf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_models/lstm_plus_attention_ustf_tfc.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lstm_attention_ustf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_lstm_attention_ustf_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model_lstm_attention_ustf_result' is not defined"],"ename":"NameError","evalue":"name 'model_lstm_attention_ustf_result' is not defined","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"#import the model\nmodel_lstm_attention_ustf,result=model_initiation(df_ustf_tfc)\nmodel_lstm_attention_ustf.load_weights(\"lstm_models/lstm_plus_attention_ustf_tfc.weights.h5\")\nevaluation(model_lstm_attention_ustf,result,\"model_lstm_attention_ustf_result\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T07:48:39.645786Z","iopub.execute_input":"2025-09-28T07:48:39.646267Z","iopub.status.idle":"2025-09-28T07:49:08.841377Z","shell.execute_reply.started":"2025-09-28T07:48:39.646245Z","shell.execute_reply":"2025-09-28T07:49:08.840724Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"I0000 00:00:1759045728.397055     124 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3035/3035\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step\n{\n  \"acc\": 0.9976419708592905,\n  \"weighted_pre\": 0.9976447622662122,\n  \"weighted_rec\": 0.9976419708592905,\n  \"weighted_f1\": 0.997641397462687,\n  \"cm\": [\n    [\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      1,\n      0,\n      4994,\n      3,\n      0,\n      1,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      1,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      1,\n      4999,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      1,\n      0,\n      0,\n      4871,\n      2,\n      0,\n      1,\n      0,\n      0,\n      0,\n      123,\n      0,\n      0,\n      2\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      7,\n      4992,\n      0,\n      0,\n      0,\n      0,\n      0,\n      1,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      2,\n      0,\n      0,\n      0,\n      0,\n      0,\n      2113,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      0,\n      1,\n      0,\n      0,\n      0,\n      0,\n      2,\n      0,\n      0,\n      75,\n      4,\n      0,\n      0,\n      0,\n      0,\n      0,\n      4917,\n      0,\n      0,\n      1\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000,\n      0\n    ],\n    [\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      0,\n      5000\n    ]\n  ]\n}\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'acc': 0.9976419708592905,\n 'weighted_pre': 0.9976447622662122,\n 'weighted_rec': 0.9976419708592905,\n 'weighted_f1': 0.997641397462687,\n 'cm': [[5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 1, 0, 4994, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 4999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 4871, 2, 0, 1, 0, 0, 0, 123, 0, 0, 2],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 4992, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2113, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0, 0, 0],\n  [0, 1, 0, 0, 0, 0, 2, 0, 0, 75, 4, 0, 0, 0, 0, 0, 4917, 0, 0, 1],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5000]]}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Train the model ciciot \nmodel_lstm_attention_ciciot,result_ciciot=model_initiation(df_ciciot)\nmodel_lstm_attention_ciciot.summary()\nhistory = model_lstm_attention_ciciot.fit(\n     result_ciciot[\"X_train_pad\"], result_ciciot[\"y_train_onehot\"],\n     validation_split=0.2,\n     epochs=50,\n     batch_size=32,\n     callbacks=callbacks\n )\nmodel_lstm_attention_ciciot.save_weights(\"lstm_models/lstm_plus_attention_ciciot.weights.h5\")\n\nevaluation(model_lstm_attention_ciciot,result_ciciot,\"model_lstm_attention_ciciot_result\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T20:27:46.191917Z","iopub.execute_input":"2025-08-09T20:27:46.192771Z","iopub.status.idle":"2025-08-09T20:50:43.979247Z","shell.execute_reply.started":"2025-08-09T20:27:46.192744Z","shell.execute_reply":"2025-08-09T20:50:43.978362Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_1             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m4,194,368\u001b[0m â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚        \u001b[38;5;34m197,632\u001b[0m â”‚ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_3           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m164,352\u001b[0m â”‚ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention_1    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m263,808\u001b[0m â”‚ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       â”‚\nâ”‚ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚                        â”‚                â”‚ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_1 (\u001b[38;5;33mAdd\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ multi_head_attention_â€¦ â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization_1     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚            \u001b[38;5;34m256\u001b[0m â”‚ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\nâ”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1dâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_1â€¦ â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚         \u001b[38;5;34m16,512\u001b[0m â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚            \u001b[38;5;34m774\u001b[0m â”‚ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_1             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> â”‚ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_3           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention_1    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> â”‚ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚                        â”‚                â”‚ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_head_attention_â€¦ â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization_1     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1dâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_1â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> â”‚ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,837,702\u001b[0m (18.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,837,702</span> (18.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m643,334\u001b[0m (2.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">643,334</span> (2.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,194,368\u001b[0m (16.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> (16.00 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754771282.588394     100 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.8604 - loss: 0.3759 - precision_1: 0.8931 - recall_1: 0.8301 - val_accuracy: 0.1630 - val_loss: 9.9135 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9725 - loss: 0.0787 - precision_1: 0.9731 - recall_1: 0.9721 - val_accuracy: 0.1620 - val_loss: 11.7892 - val_precision_1: 0.1620 - val_recall_1: 0.1620 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9792 - loss: 0.0627 - precision_1: 0.9797 - recall_1: 0.9788 - val_accuracy: 0.1622 - val_loss: 12.5865 - val_precision_1: 0.1622 - val_recall_1: 0.1622 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9802 - loss: 0.0634 - precision_1: 0.9804 - recall_1: 0.9795 - val_accuracy: 0.1630 - val_loss: 12.5362 - val_precision_1: 0.1634 - val_recall_1: 0.1630 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9873 - loss: 0.0362 - precision_1: 0.9874 - recall_1: 0.9872 - val_accuracy: 0.1630 - val_loss: 13.9721 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9891 - loss: 0.0320 - precision_1: 0.9891 - recall_1: 0.9889 - val_accuracy: 0.1630 - val_loss: 15.5006 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 5.0000e-04\nEpoch 7/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9901 - loss: 0.0309 - precision_1: 0.9903 - recall_1: 0.9898 - val_accuracy: 0.1630 - val_loss: 13.5788 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 5.0000e-04\nEpoch 8/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9927 - loss: 0.0224 - precision_1: 0.9927 - recall_1: 0.9927 - val_accuracy: 0.1630 - val_loss: 16.9151 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 2.5000e-04\nEpoch 9/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9933 - loss: 0.0233 - precision_1: 0.9933 - recall_1: 0.9931 - val_accuracy: 0.1630 - val_loss: 17.9359 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 2.5000e-04\nEpoch 10/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9944 - loss: 0.0165 - precision_1: 0.9944 - recall_1: 0.9944 - val_accuracy: 0.1630 - val_loss: 18.6838 - val_precision_1: 0.1631 - val_recall_1: 0.1630 - learning_rate: 2.5000e-04\nEpoch 11/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9959 - loss: 0.0120 - precision_1: 0.9959 - recall_1: 0.9959 - val_accuracy: 0.1630 - val_loss: 19.0015 - val_precision_1: 0.1632 - val_recall_1: 0.1630 - learning_rate: 1.2500e-04\nEpoch 12/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9966 - loss: 0.0109 - precision_1: 0.9966 - recall_1: 0.9965 - val_accuracy: 0.1630 - val_loss: 20.4564 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.2500e-04\nEpoch 13/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.0083 - precision_1: 0.9972 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 20.6877 - val_precision_1: 0.1631 - val_recall_1: 0.1630 - learning_rate: 1.2500e-04\nEpoch 14/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.0084 - precision_1: 0.9972 - recall_1: 0.9970 - val_accuracy: 0.1630 - val_loss: 23.0448 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 6.2500e-05\nEpoch 15/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9958 - loss: 0.0106 - precision_1: 0.9959 - recall_1: 0.9958 - val_accuracy: 0.1630 - val_loss: 22.9127 - val_precision_1: 0.1631 - val_recall_1: 0.1630 - learning_rate: 6.2500e-05\nEpoch 16/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9973 - loss: 0.0083 - precision_1: 0.9973 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 23.9931 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 6.2500e-05\nEpoch 17/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9975 - loss: 0.0071 - precision_1: 0.9975 - recall_1: 0.9974 - val_accuracy: 0.1630 - val_loss: 24.4991 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.1250e-05\nEpoch 18/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9974 - loss: 0.0063 - precision_1: 0.9974 - recall_1: 0.9973 - val_accuracy: 0.1630 - val_loss: 25.0247 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.1250e-05\nEpoch 19/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.0071 - precision_1: 0.9972 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 25.3645 - val_precision_1: 0.1631 - val_recall_1: 0.1630 - learning_rate: 3.1250e-05\nEpoch 20/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9970 - loss: 0.0078 - precision_1: 0.9970 - recall_1: 0.9968 - val_accuracy: 0.1630 - val_loss: 25.4907 - val_precision_1: 0.1631 - val_recall_1: 0.1630 - learning_rate: 1.5625e-05\nEpoch 21/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9976 - loss: 0.0061 - precision_1: 0.9976 - recall_1: 0.9976 - val_accuracy: 0.1630 - val_loss: 25.7153 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.5625e-05\nEpoch 22/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9973 - loss: 0.0067 - precision_1: 0.9973 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 26.1958 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.5625e-05\nEpoch 23/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9974 - loss: 0.0060 - precision_1: 0.9974 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 26.2896 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 7.8125e-06\nEpoch 24/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.0059 - precision_1: 0.9973 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 26.3533 - val_precision_1: 0.1631 - val_recall_1: 0.1630 - learning_rate: 7.8125e-06\nEpoch 25/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9975 - loss: 0.0059 - precision_1: 0.9976 - recall_1: 0.9975 - val_accuracy: 0.1630 - val_loss: 26.4721 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 7.8125e-06\nEpoch 26/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9976 - loss: 0.0055 - precision_1: 0.9977 - recall_1: 0.9976 - val_accuracy: 0.1630 - val_loss: 26.5942 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.9063e-06\nEpoch 27/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9978 - loss: 0.0055 - precision_1: 0.9978 - recall_1: 0.9978 - val_accuracy: 0.1630 - val_loss: 26.6272 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.9063e-06\nEpoch 28/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9977 - loss: 0.0052 - precision_1: 0.9977 - recall_1: 0.9977 - val_accuracy: 0.1630 - val_loss: 26.6691 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.9063e-06\nEpoch 29/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9976 - loss: 0.0056 - precision_1: 0.9976 - recall_1: 0.9975 - val_accuracy: 0.1630 - val_loss: 26.6837 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.9531e-06\nEpoch 30/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9976 - loss: 0.0058 - precision_1: 0.9976 - recall_1: 0.9976 - val_accuracy: 0.1630 - val_loss: 26.6541 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.9531e-06\nEpoch 31/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9970 - loss: 0.0068 - precision_1: 0.9973 - recall_1: 0.9970 - val_accuracy: 0.1630 - val_loss: 26.7184 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.9531e-06\nEpoch 32/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9974 - loss: 0.0058 - precision_1: 0.9974 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 26.7379 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 9.7656e-07\nEpoch 33/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.0059 - precision_1: 0.9972 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 26.7553 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 9.7656e-07\nEpoch 34/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9969 - loss: 0.0068 - precision_1: 0.9969 - recall_1: 0.9968 - val_accuracy: 0.1630 - val_loss: 26.7622 - val_precision_1: 0.1631 - val_recall_1: 0.1630 - learning_rate: 9.7656e-07\nEpoch 35/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9971 - loss: 0.0060 - precision_1: 0.9972 - recall_1: 0.9971 - val_accuracy: 0.1630 - val_loss: 26.7805 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 4.8828e-07\nEpoch 36/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9980 - loss: 0.0048 - precision_1: 0.9980 - recall_1: 0.9979 - val_accuracy: 0.1630 - val_loss: 26.7910 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 4.8828e-07\nEpoch 37/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9978 - loss: 0.0051 - precision_1: 0.9978 - recall_1: 0.9978 - val_accuracy: 0.1630 - val_loss: 26.7936 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 4.8828e-07\nEpoch 38/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9978 - loss: 0.0052 - precision_1: 0.9978 - recall_1: 0.9977 - val_accuracy: 0.1630 - val_loss: 26.7976 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 2.4414e-07\nEpoch 39/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9979 - loss: 0.0052 - precision_1: 0.9979 - recall_1: 0.9978 - val_accuracy: 0.1630 - val_loss: 26.7998 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 2.4414e-07\nEpoch 40/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9975 - loss: 0.0058 - precision_1: 0.9976 - recall_1: 0.9975 - val_accuracy: 0.1630 - val_loss: 26.8047 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 2.4414e-07\nEpoch 41/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9977 - loss: 0.0060 - precision_1: 0.9978 - recall_1: 0.9977 - val_accuracy: 0.1630 - val_loss: 26.8062 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.2207e-07\nEpoch 42/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9972 - loss: 0.0060 - precision_1: 0.9972 - recall_1: 0.9972 - val_accuracy: 0.1630 - val_loss: 26.8080 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.2207e-07\nEpoch 43/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9980 - loss: 0.0051 - precision_1: 0.9980 - recall_1: 0.9980 - val_accuracy: 0.1630 - val_loss: 26.8116 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.2207e-07\nEpoch 44/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9979 - loss: 0.0051 - precision_1: 0.9979 - recall_1: 0.9979 - val_accuracy: 0.1630 - val_loss: 26.8123 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 6.1035e-08\nEpoch 45/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9978 - loss: 0.0056 - precision_1: 0.9978 - recall_1: 0.9977 - val_accuracy: 0.1630 - val_loss: 26.8126 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 6.1035e-08\nEpoch 46/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9970 - loss: 0.0062 - precision_1: 0.9970 - recall_1: 0.9970 - val_accuracy: 0.1630 - val_loss: 26.8126 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 6.1035e-08\nEpoch 47/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9976 - loss: 0.0060 - precision_1: 0.9976 - recall_1: 0.9975 - val_accuracy: 0.1630 - val_loss: 26.8130 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.0518e-08\nEpoch 48/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9974 - loss: 0.0059 - precision_1: 0.9974 - recall_1: 0.9973 - val_accuracy: 0.1630 - val_loss: 26.8137 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.0518e-08\nEpoch 49/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9970 - loss: 0.0072 - precision_1: 0.9970 - recall_1: 0.9970 - val_accuracy: 0.1630 - val_loss: 26.8138 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 3.0518e-08\nEpoch 50/50\n\u001b[1m749/749\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9977 - loss: 0.0060 - precision_1: 0.9977 - recall_1: 0.9976 - val_accuracy: 0.1630 - val_loss: 26.8141 - val_precision_1: 0.1630 - val_recall_1: 0.1630 - learning_rate: 1.5259e-08\n\u001b[1m936/936\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step\n{\n  \"acc\": 0.8100504527381469,\n  \"weighted_pre\": 0.7104348352760586,\n  \"weighted_rec\": 0.8100504527381469,\n  \"weighted_f1\": 0.7490522788632759,\n  \"cm\": [\n    [\n      4527,\n      0,\n      414,\n      0,\n      7,\n      0\n    ],\n    [\n      32,\n      4950,\n      0,\n      0,\n      0,\n      0\n    ],\n    [\n      162,\n      0,\n      4836,\n      0,\n      1,\n      0\n    ],\n    [\n      0,\n      2,\n      0,\n      4998,\n      0,\n      0\n    ],\n    [\n      0,\n      67,\n      0,\n      0,\n      4933,\n      0\n    ],\n    [\n      1503,\n      0,\n      3489,\n      0,\n      8,\n      0\n    ]\n  ]\n}\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'acc': 0.8100504527381469,\n 'weighted_pre': 0.7104348352760586,\n 'weighted_rec': 0.8100504527381469,\n 'weighted_f1': 0.7490522788632759,\n 'cm': [[4527, 0, 414, 0, 7, 0],\n  [32, 4950, 0, 0, 0, 0],\n  [162, 0, 4836, 0, 1, 0],\n  [0, 2, 0, 4998, 0, 0],\n  [0, 67, 0, 0, 4933, 0],\n  [1503, 0, 3489, 0, 8, 0]]}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Train the model tor\nmodel_lstm_attention_tor,result_tor=model_initiation(df_tor)\nmodel_lstm_attention_tor.summary()\nhistory = model_lstm_attention_tor.fit(\n     result_tor[\"X_train_pad\"], result_tor[\"y_train_onehot\"],\n     validation_split=0.2,\n     epochs=50,\n     batch_size=32,\n     callbacks=callbacks\n )\nmodel_lstm_attention_tor.save_weights(\"lstm_models/lstm_plus_attention_tor.weights.h5\")\n\nevaluation(model_lstm_attention_tor,result_tor,\"model_lstm_attention_tor_result\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T11:41:37.034263Z","iopub.execute_input":"2025-08-10T11:41:37.034994Z","iopub.status.idle":"2025-08-10T12:09:23.587048Z","shell.execute_reply.started":"2025-08-10T11:41:37.03497Z","shell.execute_reply":"2025-08-10T12:09:23.586477Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"I0000 00:00:1754826105.216983      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1754826105.217744      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚      \u001b[38;5;34m4,194,368\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚        \u001b[38;5;34m197,632\u001b[0m â”‚ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m164,352\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m263,808\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       â”‚\nâ”‚ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚                        â”‚                â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (\u001b[38;5;33mAdd\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ multi_head_attention[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚            \u001b[38;5;34m256\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\nâ”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1d  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚         \u001b[38;5;34m16,512\u001b[0m â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              â”‚          \u001b[38;5;34m1,032\u001b[0m â”‚ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> â”‚ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attention      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚                        â”‚                â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                           â”‚                        â”‚                â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling1d  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  â”‚                        â”‚                â”‚                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> â”‚ global_average_poolinâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> â”‚ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,837,960\u001b[0m (18.46 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,837,960</span> (18.46 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m643,592\u001b[0m (2.46 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">643,592</span> (2.46 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,194,368\u001b[0m (16.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> (16.00 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754826116.005744     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 33ms/step - accuracy: 0.6180 - loss: 0.9704 - precision: 0.9215 - recall: 0.4773 - val_accuracy: 0.0624 - val_loss: 6.5639 - val_precision: 0.2262 - val_recall: 0.0624 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 32ms/step - accuracy: 0.7025 - loss: 0.7163 - precision: 0.9836 - recall: 0.5771 - val_accuracy: 0.0624 - val_loss: 8.5452 - val_precision: 0.2110 - val_recall: 0.0622 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 32ms/step - accuracy: 0.7046 - loss: 0.7158 - precision: 0.9831 - recall: 0.5752 - val_accuracy: 0.0622 - val_loss: 7.8534 - val_precision: 0.2895 - val_recall: 0.0622 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 32ms/step - accuracy: 0.7040 - loss: 0.7050 - precision: 0.9854 - recall: 0.5759 - val_accuracy: 0.0624 - val_loss: 9.6164 - val_precision: 0.2077 - val_recall: 0.0622 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7061 - loss: 0.7088 - precision: 0.9827 - recall: 0.5767 - val_accuracy: 0.0624 - val_loss: 10.9176 - val_precision: 0.2078 - val_recall: 0.0624 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7056 - loss: 0.7044 - precision: 0.9833 - recall: 0.5789 - val_accuracy: 0.0624 - val_loss: 10.1253 - val_precision: 0.2136 - val_recall: 0.0622 - learning_rate: 5.0000e-04\nEpoch 7/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7075 - loss: 0.7004 - precision: 0.9848 - recall: 0.5771 - val_accuracy: 0.0624 - val_loss: 11.8823 - val_precision: 0.2061 - val_recall: 0.0624 - learning_rate: 5.0000e-04\nEpoch 8/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7095 - loss: 0.6967 - precision: 0.9840 - recall: 0.5794 - val_accuracy: 0.0622 - val_loss: 12.2049 - val_precision: 0.2101 - val_recall: 0.0622 - learning_rate: 2.5000e-04\nEpoch 9/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7078 - loss: 0.6978 - precision: 0.9837 - recall: 0.5773 - val_accuracy: 0.0625 - val_loss: 11.1886 - val_precision: 0.2061 - val_recall: 0.0624 - learning_rate: 2.5000e-04\nEpoch 10/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7076 - loss: 0.6998 - precision: 0.9831 - recall: 0.5749 - val_accuracy: 0.0627 - val_loss: 12.8163 - val_precision: 0.2060 - val_recall: 0.0626 - learning_rate: 2.5000e-04\nEpoch 11/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7091 - loss: 0.6920 - precision: 0.9849 - recall: 0.5803 - val_accuracy: 0.0624 - val_loss: 12.5284 - val_precision: 0.2057 - val_recall: 0.0624 - learning_rate: 1.2500e-04\nEpoch 12/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7107 - loss: 0.6882 - precision: 0.9835 - recall: 0.5809 - val_accuracy: 0.0624 - val_loss: 12.9903 - val_precision: 0.2052 - val_recall: 0.0624 - learning_rate: 1.2500e-04\nEpoch 13/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7072 - loss: 0.6938 - precision: 0.9833 - recall: 0.5780 - val_accuracy: 0.0627 - val_loss: 12.6127 - val_precision: 0.2061 - val_recall: 0.0625 - learning_rate: 1.2500e-04\nEpoch 14/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7118 - loss: 0.6834 - precision: 0.9851 - recall: 0.5826 - val_accuracy: 0.0627 - val_loss: 13.2592 - val_precision: 0.2048 - val_recall: 0.0626 - learning_rate: 6.2500e-05\nEpoch 15/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7066 - loss: 0.6904 - precision: 0.9824 - recall: 0.5801 - val_accuracy: 0.0627 - val_loss: 13.7170 - val_precision: 0.2047 - val_recall: 0.0626 - learning_rate: 6.2500e-05\nEpoch 16/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7082 - loss: 0.6890 - precision: 0.9834 - recall: 0.5820 - val_accuracy: 0.0627 - val_loss: 14.0499 - val_precision: 0.2049 - val_recall: 0.0626 - learning_rate: 6.2500e-05\nEpoch 17/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7085 - loss: 0.6918 - precision: 0.9842 - recall: 0.5776 - val_accuracy: 0.0627 - val_loss: 14.1821 - val_precision: 0.2046 - val_recall: 0.0627 - learning_rate: 3.1250e-05\nEpoch 18/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7096 - loss: 0.6883 - precision: 0.9846 - recall: 0.5792 - val_accuracy: 0.0629 - val_loss: 14.2617 - val_precision: 0.2039 - val_recall: 0.0627 - learning_rate: 3.1250e-05\nEpoch 19/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7078 - loss: 0.6867 - precision: 0.9858 - recall: 0.5796 - val_accuracy: 0.0629 - val_loss: 14.2207 - val_precision: 0.2034 - val_recall: 0.0627 - learning_rate: 3.1250e-05\nEpoch 20/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7113 - loss: 0.6847 - precision: 0.9852 - recall: 0.5816 - val_accuracy: 0.0627 - val_loss: 14.5097 - val_precision: 0.2035 - val_recall: 0.0627 - learning_rate: 1.5625e-05\nEpoch 21/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7111 - loss: 0.6798 - precision: 0.9835 - recall: 0.5853 - val_accuracy: 0.0627 - val_loss: 14.7960 - val_precision: 0.2031 - val_recall: 0.0627 - learning_rate: 1.5625e-05\nEpoch 22/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7140 - loss: 0.6723 - precision: 0.9868 - recall: 0.5895 - val_accuracy: 0.0629 - val_loss: 15.0196 - val_precision: 0.2033 - val_recall: 0.0627 - learning_rate: 1.5625e-05\nEpoch 23/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7109 - loss: 0.6820 - precision: 0.9847 - recall: 0.5801 - val_accuracy: 0.0627 - val_loss: 14.9126 - val_precision: 0.2031 - val_recall: 0.0627 - learning_rate: 7.8125e-06\nEpoch 24/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7063 - loss: 0.6889 - precision: 0.9829 - recall: 0.5770 - val_accuracy: 0.0627 - val_loss: 14.9782 - val_precision: 0.2029 - val_recall: 0.0627 - learning_rate: 7.8125e-06\nEpoch 25/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7097 - loss: 0.6837 - precision: 0.9849 - recall: 0.5825 - val_accuracy: 0.0629 - val_loss: 15.0914 - val_precision: 0.2032 - val_recall: 0.0627 - learning_rate: 7.8125e-06\nEpoch 26/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7161 - loss: 0.6763 - precision: 0.9845 - recall: 0.5862 - val_accuracy: 0.0627 - val_loss: 15.0339 - val_precision: 0.2033 - val_recall: 0.0627 - learning_rate: 3.9063e-06\nEpoch 27/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7082 - loss: 0.6868 - precision: 0.9860 - recall: 0.5806 - val_accuracy: 0.0627 - val_loss: 15.0949 - val_precision: 0.2028 - val_recall: 0.0627 - learning_rate: 3.9063e-06\nEpoch 28/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7084 - loss: 0.6852 - precision: 0.9849 - recall: 0.5797 - val_accuracy: 0.0627 - val_loss: 15.0989 - val_precision: 0.2029 - val_recall: 0.0627 - learning_rate: 3.9063e-06\nEpoch 29/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7086 - loss: 0.6825 - precision: 0.9851 - recall: 0.5801 - val_accuracy: 0.0627 - val_loss: 15.1470 - val_precision: 0.2029 - val_recall: 0.0627 - learning_rate: 1.9531e-06\nEpoch 30/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7094 - loss: 0.6832 - precision: 0.9847 - recall: 0.5833 - val_accuracy: 0.0627 - val_loss: 15.1473 - val_precision: 0.2028 - val_recall: 0.0627 - learning_rate: 1.9531e-06\nEpoch 31/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7118 - loss: 0.6757 - precision: 0.9852 - recall: 0.5860 - val_accuracy: 0.0627 - val_loss: 15.1694 - val_precision: 0.2028 - val_recall: 0.0627 - learning_rate: 1.9531e-06\nEpoch 32/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7084 - loss: 0.6818 - precision: 0.9859 - recall: 0.5819 - val_accuracy: 0.0627 - val_loss: 15.1806 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 9.7656e-07\nEpoch 33/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7099 - loss: 0.6817 - precision: 0.9854 - recall: 0.5819 - val_accuracy: 0.0629 - val_loss: 15.1818 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 9.7656e-07\nEpoch 34/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7105 - loss: 0.6805 - precision: 0.9854 - recall: 0.5849 - val_accuracy: 0.0629 - val_loss: 15.1894 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 9.7656e-07\nEpoch 35/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7115 - loss: 0.6799 - precision: 0.9864 - recall: 0.5809 - val_accuracy: 0.0629 - val_loss: 15.1941 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 4.8828e-07\nEpoch 36/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7076 - loss: 0.6865 - precision: 0.9844 - recall: 0.5803 - val_accuracy: 0.0629 - val_loss: 15.1974 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 4.8828e-07\nEpoch 37/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7119 - loss: 0.6787 - precision: 0.9854 - recall: 0.5831 - val_accuracy: 0.0629 - val_loss: 15.2065 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 4.8828e-07\nEpoch 38/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7142 - loss: 0.6718 - precision: 0.9841 - recall: 0.5862 - val_accuracy: 0.0629 - val_loss: 15.2059 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 2.4414e-07\nEpoch 39/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7138 - loss: 0.6710 - precision: 0.9841 - recall: 0.5887 - val_accuracy: 0.0629 - val_loss: 15.2055 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 2.4414e-07\nEpoch 40/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7101 - loss: 0.6854 - precision: 0.9847 - recall: 0.5812 - val_accuracy: 0.0629 - val_loss: 15.2018 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 2.4414e-07\nEpoch 41/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7093 - loss: 0.6831 - precision: 0.9849 - recall: 0.5810 - val_accuracy: 0.0629 - val_loss: 15.2024 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 1.2207e-07\nEpoch 42/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7120 - loss: 0.6731 - precision: 0.9868 - recall: 0.5833 - val_accuracy: 0.0629 - val_loss: 15.2044 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 1.2207e-07\nEpoch 43/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7114 - loss: 0.6757 - precision: 0.9842 - recall: 0.5852 - val_accuracy: 0.0629 - val_loss: 15.2052 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 1.2207e-07\nEpoch 44/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7130 - loss: 0.6788 - precision: 0.9832 - recall: 0.5813 - val_accuracy: 0.0629 - val_loss: 15.2059 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 6.1035e-08\nEpoch 45/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7109 - loss: 0.6814 - precision: 0.9869 - recall: 0.5812 - val_accuracy: 0.0629 - val_loss: 15.2057 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 6.1035e-08\nEpoch 46/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7129 - loss: 0.6779 - precision: 0.9857 - recall: 0.5839 - val_accuracy: 0.0629 - val_loss: 15.2055 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 6.1035e-08\nEpoch 47/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7136 - loss: 0.6742 - precision: 0.9848 - recall: 0.5813 - val_accuracy: 0.0629 - val_loss: 15.2054 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 3.0518e-08\nEpoch 48/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7089 - loss: 0.6865 - precision: 0.9851 - recall: 0.5805 - val_accuracy: 0.0629 - val_loss: 15.2060 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 3.0518e-08\nEpoch 49/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7103 - loss: 0.6819 - precision: 0.9851 - recall: 0.5832 - val_accuracy: 0.0629 - val_loss: 15.2060 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 3.0518e-08\nEpoch 50/50\n\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.7128 - loss: 0.6805 - precision: 0.9839 - recall: 0.5822 - val_accuracy: 0.0629 - val_loss: 15.2062 - val_precision: 0.2027 - val_recall: 0.0627 - learning_rate: 1.5259e-08\n\u001b[1m1250/1250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step\n{\n  \"acc\": 0.581,\n  \"weighted_pre\": 0.684585045622129,\n  \"weighted_rec\": 0.581,\n  \"weighted_f1\": 0.5593845756262176,\n  \"cm\": [\n    [\n      4495,\n      502,\n      0,\n      0,\n      1,\n      1,\n      1,\n      0\n    ],\n    [\n      16,\n      4962,\n      2,\n      0,\n      20,\n      0,\n      0,\n      0\n    ],\n    [\n      16,\n      2643,\n      2327,\n      0,\n      14,\n      0,\n      0,\n      0\n    ],\n    [\n      2,\n      88,\n      0,\n      4909,\n      0,\n      1,\n      0,\n      0\n    ],\n    [\n      4,\n      3447,\n      5,\n      0,\n      1544,\n      0,\n      0,\n      0\n    ],\n    [\n      236,\n      618,\n      0,\n      0,\n      0,\n      4144,\n      2,\n      0\n    ],\n    [\n      7,\n      4105,\n      3,\n      0,\n      23,\n      3,\n      859,\n      0\n    ],\n    [\n      96,\n      3075,\n      8,\n      9,\n      18,\n      1162,\n      632,\n      0\n    ]\n  ]\n}\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'acc': 0.581,\n 'weighted_pre': 0.684585045622129,\n 'weighted_rec': 0.581,\n 'weighted_f1': 0.5593845756262176,\n 'cm': [[4495, 502, 0, 0, 1, 1, 1, 0],\n  [16, 4962, 2, 0, 20, 0, 0, 0],\n  [16, 2643, 2327, 0, 14, 0, 0, 0],\n  [2, 88, 0, 4909, 0, 1, 0, 0],\n  [4, 3447, 5, 0, 1544, 0, 0, 0],\n  [236, 618, 0, 0, 0, 4144, 2, 0],\n  [7, 4105, 3, 0, 23, 3, 859, 0],\n  [96, 3075, 8, 9, 18, 1162, 632, 0]]}"},"metadata":{}}],"execution_count":10}]}